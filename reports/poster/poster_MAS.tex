\documentclass[portrait,final,x11names,a1paper,fontscale=0.4]{baposter}
\usepackage{calc}
\usepackage{graphicx,caption}
\usepackage{relsize}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{bm}
\usepackage[]{hyperref} 
\usepackage{url}

\usepackage{graphicx,subfigure}
\usepackage{multicol}

\usepackage{microtype}
\usepackage{helvet}
\renewcommand{\familydefault}{\sfdefault}
\renewcommand{\ttdefault}{lmtt}

% Mathematics
\usepackage{amsmath,amsfonts,amsthm,amssymb,bm}
\usepackage{dutchcal}

\usepackage{numprint}
\npthousandsep{,}\npthousandthpartsep{}\npdecimalsign{.}	

\usepackage{mathtools}
\mathtoolsset{showonlyrefs}

\input{commands.tex}
\usepackage[font=small,labelfont=bf,textfont=it]{caption}

\setlength{\columnsep}{1.5em}
\setlength{\columnseprule}{0mm}

\usepackage{enumitem}
\setlist{noitemsep,nolistsep}
\usepackage{stackengine,fontawesome}

\usepackage{tabu}
\usepackage{multicol}
\usepackage{multirow}

\definecolor{blue_icl}{RGB}{0,62,116}
\definecolor{blue_icl2}{RGB}{0,40,255}
\definecolor{navy_icl}{RGB}{0,33,71}
\definecolor{cool_icl}{RGB}{0,110,175}

\newcommand{\icl}[1]{{\bf\color{blue_icl2}{#1}}}

\DeclareMathOperator{\argmax}{argmax}
\usepackage{bm}

\newcommand{\codepath}{./}
\usepackage{minted}
\usemintedstyle{lovelace}
\newcommand{\includeshell}[1] {\inputminted[firstline=1,fontsize=\footnotesize,breaklines]{shell-session}{\codepath/#1}}

% Vettore bold
\renewcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\mvec}[1]{\mathbf{#1}}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}
\newtcbtheorem{defi}{Definition}%
{colback=blue_icl2!5,colframe=blue_icl2,fonttitle=\bfseries,left=2pt,right=2pt,top=2pt,bottom=2pt}{th}

% Tables
\usepackage{booktabs}	
\usepackage{array,tabu}
\usepackage{multicol}
\usepackage{multirow}

\begin{document}

\begin{poster}%
  % Poster Options
  {
  % Show grid to help with alignment
  grid=false,
  % Column spacing
  colspacing=1em,
  % Color style
  bgColorOne=white,
  bgColorTwo=white,
  borderColor=blue_icl2,
  headerColorOne=cool_icl,
  headerColorTwo=blue_icl2,
  headerFontColor=white,
  boxColorOne=white,
  boxColorTwo=blue_icl2,
  % Format of textbox
  textborder=roundedleft,
  % Format of text header
  eyecatcher=true,
  headerborder=closed,
  headerheight=0.15\textheight,
  textfont=\bf,
  headershape=roundedright,
  % headershade=shadelr,
  headerfont=\Large\bf,
  textfont={\setlength{\parindent}{1.5em}},
  %boxshade=plain,
  % background=shade-tb,
  % background=plain,
  % bgColorOne=DarkOrange2!40,
  linewidth=2pt,
  columns=5
  }
  % Eye Catcher
  {\includegraphics[width=14em]{icl_eye.pdf}}%{imperial_logo.pdf}} 
  % Title
  {\centering\color{blue_icl2}{\bf{\huge Automating the selection of  preprocessing  \\[0.1cm]techniques for deep neural networks}}\vspace{0.5em}}
  % Authors
  {\vspace*{-.3cm}
  \hspace*{1.9cm} 
  \leavevmode\hbox to \linewidth{ \color{blue_icl2}% 
  \centering
\begin{tabular}[t]{c@{}}
    {\Large{\textbf{Student:} Marcus September}} \\[.05cm]
    {\Large{\textbf{Supervisor:} Francesco Sanna Passino}} \\[.05cm]
\faUniversity\ {Department of Mathematics, Imperial College London} \\
\faEnvelopeO\ {\tt marcus.september22@imperial.ac.uk} \\
%{\large This work is funded by a {\bf Microsoft Security AI} research grant.}
\end{tabular} 
}}

  % University logo
%  {% The makebox allows the title to flow into the logo, this is a hack because of the L shaped logo.
    % \includegraphics[height=9.0em]{images/logo}
%  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \headerbox{1. Problem and motivation}{name=problem,column=0,row=0,span=2}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent   
Deep learning sequence models, such as Recurrent Neural Networks (RNNs) and
transformers, are sensitive to input variable distributions. Both training
speed and performance can drop significantly for non-normal distributions, such
as skewed distributions and those with outliers.
% Real-world data also usually
% contains a lot of missing values, which needs to be converted to numeric values
% for use in deep learning models. Deciding on appropriate preprocessing methods
% and how to handle missing values is essential in optimising model performance.
Preprocessing includes all transformations applied to the data before feeding it into
the neural network, and deciding the appropriate techniques is essential for
optimising model performance.
However, this is a time-consuming process. This project aims to automate this by
automatically selecting the preprocessing methods to use for any given
sequence dataset, increasing both model performance and training efficiency.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{2. Default prediction dataset}{name=data,column=0,row=1,below=problem,span=2}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
The default prediction dataset, provided by American Express,
contains a multivariate time-series from $N\approx 460\;000$ different customers.
Each time-series has $P=188$ aggregated
profile features recorded at up to $T=13$ different credit card statement
dates.  For each multivariate time-series, the target label
$y \in \{0,1\}$ indicates whether the customer defaulted on their loan or not.
The task is to predict the probability $\mathbb{P}(Y=1)$ for each customer.
Note that due to
privacy concerns, the name of all the features have been anonymized. 
Additionally, a small amount of uniform noise has been added to all the numeric features.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{3. Exploratory data analysis}{name=eda,column=0,row=1,span=2,below=data}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %\vspace*{.2cm}
    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %\begin{minipage}[t]{0.25\linewidth}
    %\hspace*{-0.5cm}
    %\includegraphics[width=0.95\textwidth]{1920px-M_box.png}
    %\end{minipage} \ \hspace{-0.5cm}
    %\begin{minipage}[b]{0.7\linewidth} %\smaller
    \noindent
    \begin{minipage}{.975\textwidth}
        \captionof{figure}{\textbf{Histogram of 3 of the 188 variables from the default prediction dataset.}}
        \vspace*{-.3cm}
        \includegraphics[width=0.975\textwidth]{Figures/poster-eda.pdf}
        \vspace*{-.15cm}
    \end{minipage}

    \noindent
    The default prediction dataset exhibits many traits
    of real-world datasets, such as very skewed distributions, multiple modes, unusual peaks
    and extreme values.
    Across the whole dataset, 8.50\% of the numeric data points are missing.
    The 5 most incomplete variables are missing between 91.52\% and 92.62\% of the data points.
    Only 138 out of the 177 numeric variables have less than 1\% missing values. 
    All this makes the dataset ideal for evaluating how effective the proposed
    preprocessing techniques are on real-world data.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{4. Synthetic data}{name=synthdata,column=0,span=2,row=3,below=eda}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent
With a custom-made synthetic data  generation procedure, I can generate new
labelled multivariate time-series where the variables follow arbitrary distributions based on the
unnormalized PDFs I provide. This allows synthesizing data with real-world-like distributions
in a controlled matter, which makes it easier and more efficiency to experiment and learn
when each preprocessing method works best. This insight will then be used to automate the
preprocessing step.


% \begin{itemize}[leftmargin=.15in]
% \item 
% With the synthetic data generation procedure I have proposed, I can specify arbitrary unnormalized probability density functions (PDFs) for each of the $P$ variables, from which the inverse cumulative
% density functions, $F^{-1}_1,\dots,F^{-1}_P$ are inferred. Correlated uniform random variables
% $U \in [0,1]^{T \times P}$
% with a similar correlation structure as that of a multivariate time-series are then generated,
% and a response $y \in \{0,1\}$ is formed from these. These uniform random variables are then
% transformed by $F^{-1}_1,\dots,F^{-1}_P$ and returned to the user with the response, as samples from the provided PDFs.
% % TODO: could maybe replace this with a diagram of $U$s, arrow down to $X$s, and arrow right to response. Could also shorten the explanation to just emphasize fact that I can specify the PDFs
% \item  By being able to fully control the distribution of each variable, it will be easier to experiment and learn \textit{when} each preprocessing method works best, and this insight can be used to
%     form the heuristics used for automatically selection the appropriate techniques.
% \end{itemize}


}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{5. Preliminary result I: Preprocessing on real data}{name=picture,column=2,span=3,row=1,aligned=problem}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{itemize}[leftmargin=.15in]
%     \item Applying appropriate preprocessing can improve both the predictive performance and the training efficiency of deep sequence models (see figure 1a).
%     \item Non-normalized data reduces the predictive power of deep sequence models (see figure 1b/table)
% \end{itemize}

\noindent
\begin{minipage}{.975\textwidth}
\captionof{figure}{\textbf{Average 5-fold cross-validation loss for different preprocessing
techniques \\ applied on the American Express default prediction dataset, using a GRU RNN model}}
\vspace*{-.25cm}
\includegraphics[width=0.975\textwidth]{Figures/poster-val-loss.pdf}
\vspace*{-.25cm}
\end{minipage}
\noindent
Applying an appropriate preprocessing technique before training can significantly improve
performance. Additionally, with the right preprocessing technique the number of epochs required for convergence is
reduced, as seen in figure 2.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{6. Preliminary result II: Using synthetic data}{name=results2,column=2,span=3,below=picture}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
Using the synthetic data generation procedure, two multivariate time-series datasets of dimensions $(N=10\;000,T=6,D=2)$ were
generated. The first one was configured to generate data with very skewed and irregular distributions, containing outliers, while the
variables in the second dataset were all normally distributed. 
\noindent
\begin{center}
    \vspace*{-.5cm}
\captionof{table}{\textbf{Performance metrics after training a GRU RNN model on synthetic data with specified variable distributions (Sample size 50, and results presented with a 95\% CI)}}
\begin{tabular}{c|c|c|c}
    \toprule
    Variable distributions & Validation loss & AMEX metric & Binary accuracy \\
    \midrule
    Non-normal & $0.2209 \pm 0.0183$ & $0.7907 \pm 0.0373$ & $90.53\% \pm 1.13\%$ \\
    Normal & $ 0.1356 \pm 0.0176$ & $ 0.8683 \pm 0.0242$ & $ 94.19\% \pm 1.12\%$ \\
    \bottomrule
    % Using non-normal distributions
    % Average validation loss: 0.2209 +- 0.0183
    % Average metrics: 0.7907 +- 0.0373
    % Average accuracy: 0.9053 +- 0.0113
    % Number of epochs: 16.9800 +- 7.3701% % Using normally distributed data
    % Average validation loss: 0.1356 +- 0.0176
    % Average metrics: 0.8683 +- 0.0242
    % Average accuracy: 0.9419 +- 0.0112
    % Number of epochs: 13.2000 +- 6.4531
    % Sample size 50
% \toprule
% Dataset & Validation accuracy  \\
% \midrule
% Synthetic data with non-normal distributions & 95.65\%  \\
% Synthetic data with uniform distributions & 98.65\% \\
% \bottomrule
\end{tabular}
\end{center}
From table 1, we can conclude there exists suitable variable transformations that can be applied to the data to
significantly increase performance.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{7. Automating the preprocessing procedure}{name=results,column=2,span=3,below=results2,above=bottom}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent
\begin{minipage}{.975\textwidth}
    \captionof{figure}{\textbf{Proposition I: Automating preprocessing using heuristics and determining suitable static transformations for each variable based on these}}
    \vspace*{-.3cm}
    \begin{center}
    \includegraphics[width=0.975\textwidth]{Figures/automated-preprocessing-heuristics.pdf} 
    \end{center}
\end{minipage}
\begin{minipage}{.975\textwidth}
    \captionof{figure}{\textbf{Proposition II: Automating preprocessing using an adaptive preprocessing layer with weights that are
    learned during training}}
    \vspace*{-.15cm}
    \begin{center}
    \includegraphics[width=\textwidth]{Figures/automated-preprocessing-adaptive.pdf} 
    \end{center}
    \vspace*{-.15cm}
\end{minipage}

\noindent
To automate the selection of preprocessing techniques, there are two methods I propose to investigate
further. One option, illustrated in figure 3, is based on heuristics. With the second option, as shown in figure 4,
the preprocessing methods are parameterized and learned as part of the training procedure.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \headerbox{References}{name=conclusion,column=2,span=3,above=bottom, below=results}{
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %\begin{itemize}[leftmargin=.15in] 
% %\item \icl{\underline{Latent structure blockmodels}}
% %\begin{itemize}[leftmargin=.1cm] 
% %\item A model for graphs with group-specific manifold structure;
% %\item Gaussian process priors on the unknown latent functions;
% %\item Common clustering models are special cases of LSBMs.
% %\end{itemize}
% %\end{itemize}
% {\footnotesize{
% \begin{itemize}[leftmargin=.1in] 
% % \item Athreya, A. et al. (2018). “Statistical Inference on Random Dot Product Graphs: a Survey”. Journal of Machine Learning Research 18, 1--92.
% % \item Athreya, A. et al. (2021). “On Estimation and Inference in Latent Structure Random Graphs”. Statistical Science 36.1, 68--88.
% % \item Hoff, P. D. et al. (2002). “Latent space approaches to social network analysis”. Journal of the American Statistical Association 97, 1090--1098. 
% % %\item Holland, P. W., K. B. Laskey, and S. Leinhardt (1983). “Stochastic blockmodels: First steps”. Social Networks 5.2, 109--137.
% % %\item Karrer, B. and M. E. J. Newman (2011). “Stochastic blockmodels and community structure in networks”. Physical Review E 83 (1).
% % \item Rubin-Delanchy, P. (2020). “Manifold structure in graph embeddings”. Advances in Neural Information Processing Systems 33, %Ed. by H. Larochelle et al. Vol. 33. Curran Associates, Inc.,  
% % 11687–11699.
% \end{itemize}
% }}
% }

\end{poster}

\end{document}

