%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%              References file           %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% For the references some tips:
% * Remove all the DOIs and URLs
% * Look up the ISBN number of the work if not present, and add this.
% * If too many authors in one reference, manually remove some and replace with "et. al."

% Outliers, winsorization
@article{winsorization,
    title = {The effects of handling outliers on the performance of bankruptcy prediction models},
    journal = {Socio-Economic Planning Sciences},
    volume = {67},
    pages = {34-42},
    year = {2019},
    issn = {0038-0121},
    doi = {https://doi.org/10.1016/j.seps.2018.08.004},
    url = {https://www.sciencedirect.com/science/article/pii/S003801211730232X},
    author = {Tamás Nyitrai and Miklós Virág},
    keywords = {Bankruptcy prediction, Data preprocessing, Winsorizing, Decision trees, CHAID, CART, Neural networks}
}

% For the change of variables formula
@book{murphy,
    added-at = {2017-02-27T11:22:42.000+0100},
    address = {Cambridge, Mass. [u.a.]},
    author = {Murphy, Kevin P.},
    biburl = {https://www.bibsonomy.org/bibtex/270148d65a6a66e0ae962bf22c5f66148/hotho},
    description = {Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series): Kevin P. Murphy: 9780262018029: Amazon.com: Books},
    interhash = {e99d8a06cc36507b05c38192ab80573e},
    intrahash = {70148d65a6a66e0ae962bf22c5f66148},
    isbn = {9780262018029 0262018020},
    keywords = {hmm lda learning machine statistics},
    publisher = {MIT Press},
    refid = {904442949},
    timestamp = {2017-02-27T11:22:42.000+0100},
    title = {Machine learning : a probabilistic perspective},
    url = {https://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/ref=sr_1_2?ie=UTF8&qid=1336857747&sr=8-2},
    year = 2013
}
@article{normalizing_flows,
    doi = {10.1109/tpami.2020.2992934},
    url = {https://doi.org/10.1109%2Ftpami.2020.2992934},
    year = 2021,
    month = {nov},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    volume = {43},
    number = {11},
    pages = {3964--3979},
    author = {Ivan Kobyzev and Simon J.D. Prince and Marcus A. Brubaker},
    title = {Normalizing Flows: An Introduction and Review of Current Methods},
    journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence}
}

@article{yeoJohnson,
    ISSN = {00063444},
    URL = {http://www.jstor.org/stable/2673623},
    abstract = {We introduce a new power transformation family which is well defined on the whole real line and which is appropriate for reducing skewness and to approximate normality. It has properties similar to those of the Box-Cox transformation for positive variables. The large-sample properties of the transformation are investigated in the contect of a single random sample},
    author = {In-Kwon Yeo and Richard A. Johnson},
    journal = {Biometrika},
    number = {4},
    pages = {954--959},
    publisher = {[Oxford University Press, Biometrika Trust]},
    title = {A New Family of Power Transformations to Improve Normality or Symmetry},
    urldate = {2023-08-04},
    volume = {87},
    year = {2000}
}

@article{rdain,
    author={Passalis, Nikolaos
            and Kanniainen, Juho
            and Gabbouj, Moncef
            and Iosifidis, Alexandros
            and Tefas, Anastasios},
    title={Forecasting Financial Time Series Using Robust Deep Adaptive Input Normalization},
    journal={Journal of Signal Processing Systems},
    year={2021},
    month={Oct},
    day={01},
    volume={93},
    number={10},
    pages={1235-1251},
    issn={1939-8115},
    doi={10.1007/s11265-020-01624-0},
    url={https://doi.org/10.1007/s11265-020-01624-0}
}

@article{dain,
    title={Deep Adaptive Input Normalization for Time Series Forecasting}, 
    author={Nikolaos Passalis and Anastasios Tefas and Juho Kanniainen and Moncef Gabbouj and Alexandros Iosifidis},
    year={2019},
    eprint={1902.07892},
    journal={arXiv preprint arXiv:1902.07892},
    archivePrefix={arXiv},
    primaryClass={q-fin.CP}
}

@article{bin,
    title={Bilinear Input Normalization for Neural Networks in Financial Forecasting}, 
    author={Dat Thanh Tran and Juho Kanniainen and Moncef Gabbouj and Alexandros Iosifidis},
    year={2021},
    journal={arXiv preprint arXiv:2109.00983},
    archivePrefix={arXiv},
    primaryClass={q-fin.ST}
}

@article{mixture_ct,
    author = {Cao, Zheng and Gao, Xiang and Chang, Yankui and Liu, Gongfa and Pei, Yuanji},
    year = {2023},
    month = {04},
    pages = {e14004},
    title = {Improving synthetic CT accuracy by combining the benefits of multiple normalized preprocesses},
    journal = {Journal of applied clinical medical physics},
    doi = {10.1002/acm2.14004}
}

@article{preprocess_origin,
    author = {Sola, J. and Sevilla, Joaquin},
    year = {1997},
    month = {07},
    pages = {1464 - 1468},
    title = {Importance of input data normalization for the application of neural networks to complex industrial problems},
    volume = {44},
    journal = {Nuclear Science, IEEE Transactions on},
    doi = {10.1109/23.589532}
}

@article{singh,
    title = {Investigating the impact of data normalization on classification performance},
    journal = {Applied Soft Computing},
    volume = {97},
    pages = {105524},
    year = {2020},
    issn = {1568-4946},
    doi = {https://doi.org/10.1016/j.asoc.2019.105524},
    url = {https://www.sciencedirect.com/science/article/pii/S1568494619302947},
    author = {Dalwinder Singh and Birmohan Singh},
    keywords = {Ant lion optimization, Data normalization, Feature selection, Feature weighting, -NN classifier}
}

@article{nawi,
    title = {The Effect of Data Pre-processing on Optimized Training of Artificial Neural Networks},
    journal = {Procedia Technology},
    volume = {11},
    pages = {32-39},
    year = {2013},
    note = {4th International Conference on Electrical Engineering and Informatics, ICEEI 2013},
    issn = {2212-0173},
    doi = {https://doi.org/10.1016/j.protcy.2013.12.159},
    url = {https://www.sciencedirect.com/science/article/pii/S2212017313003137},
    author = {Nazri Mohd Nawi and Walid Hasen Atomi and M.Z. Rehman},
    keywords = {Artificial neural networks, back propagation, gradient descent, gain value, pre-processing data}
}

@inproceedings{stanislav,
    author={Koval, Stanislav I.},
    booktitle={2018 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)}, 
    title={Data preparation for neural network data analysis}, 
    year={2018},
    volume={},
    number={},
    pages={898-901},
    doi={10.1109/EIConRus.2018.8317233}
}


@article{boxcox,
    ISSN = {00359246},
    URL = {http://www.jstor.org/stable/2984418},
    abstract = {In the analysis of data it is often assumed that observations y1, y2, ..., yn are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters θ. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
    author = {G. E. P. Box and D. R. Cox},
    journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
    number = {2},
    pages = {211--252},
    publisher = {[Royal Statistical Society, Wiley]},
    title = {An Analysis of Transformations},
    urldate = {2023-08-07},
    volume = {26},
    year = {1964}
}

@misc{shape,
    title = {Measures of Shape: Skewness and Kurtosis},
    howpublished = {\url{http://brownmath.com/stat/shape.htm}},
    note = {Accessed: 2023-08-09},
    author = {Stan Brown},
    year = 2022,
    urldate = {2023-08-09},
}

@Inbook{kmeans,
    author="Jin, Xin
    and Han, Jiawei",
    editor="Sammut, Claude
    and Webb, Geoffrey I.",
    title="K-Means Clustering",
    bookTitle="Encyclopedia of Machine Learning",
    year="2010",
    publisher="Springer US",
    address="Boston, MA",
    pages="563--564",
    isbn="978-0-387-30164-8",
    doi="10.1007/978-0-387-30164-8_425",
    url="https://doi.org/10.1007/978-0-387-30164-8_425"
}


@book{mackay,
    added-at = {2007-05-24T14:43:04.000+0200},
    author = {MacKay, David J. C.},
    biburl = {https://www.bibsonomy.org/bibtex/24c23fea472f6e75c0964badd83883d77/tmalsburg},
    interhash = {86f621d9d6f9f159448f768d792d4511},
    intrahash = {4c23fea472f6e75c0964badd83883d77},
    keywords = {bayesianinference book informationtheory neuralnetworks patternrecognition probabilitytheory},
    publisher = {Copyright Cambridge University Press},
    timestamp = {2007-05-24T14:43:04.000+0200},
    title = {Information Theory, Inference, and Learning Algorithms},
    year = 2003
}

@misc{hierarchical_clustering,
    author = "{Wikipedia contributors}",
    title = "Hierarchical clustering --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2023",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Hierarchical_clustering&oldid=1163178180}",
    note = "[Online; accessed 9-August-2023]"
}

@inproceedings{gru,
    title = "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    author = "Junyoung Chung and Caglar Gulcehre and Kyunghyun Cho and Yoshua Bengio",
    year = "2014",
    language = "English (US)",
    booktitle = "NIPS 2014 Workshop on Deep Learning, December 2014",

}

@article{dnn,
    doi = {10.1016/j.neunet.2014.09.003},
    url = {https://doi.org/10.1016%2Fj.neunet.2014.09.003},
    year = 2015,
    month = {jan},
    publisher = {Elsevier {BV}},
    volume = {61},
    pages = {85--117},
    author = {Jürgen Schmidhuber},
    title = {Deep learning in neural networks: An overview},
    journal = {Neural Networks}
}

@inproceedings{backprop,
    author={Hecht-Nielsen},
    booktitle={International 1989 Joint Conference on Neural Networks}, 
    title={Theory of the backpropagation neural network}, 
    year={1989},
    volume={},
    number={},
    pages={593-605 vol.1},
    doi={10.1109/IJCNN.1989.118638}
}

@incollection{pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
    booktitle = {Advances in Neural Information Processing Systems 32},
    pages = {8024--8035},
    year = {2019},
    publisher = {Curran Associates, Inc.},
    url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{tensorflow,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    note={Software available from tensorflow.org},
    author={
            Mart\'{i}n~Abadi and
            Ashish~Agarwal and
            Paul~Barham and
            Eugene~Brevdo and
            Zhifeng~Chen and
            Craig~Citro and
            Greg~S.~Corrado and
            Andy~Davis and
            Jeffrey~Dean and
            Matthieu~Devin and
            Sanjay~Ghemawat and
            Ian~Goodfellow and
            Andrew~Harp and
            Geoffrey~Irving and
            Michael~Isard and
            Yangqing Jia and
            Rafal~Jozefowicz and
            Lukasz~Kaiser and
            Manjunath~Kudlur and
            Josh~Levenberg and
            Dandelion~Man\'{e} and
            Rajat~Monga and
            Sherry~Moore and
            Derek~Murray and
            Chris~Olah and
            Mike~Schuster and
            Jonathon~Shlens and
            Benoit~Steiner and
            Ilya~Sutskever and
            Kunal~Talwar and
            Paul~Tucker and
            Vincent~Vanhoucke and
            Vijay~Vasudevan and
            Fernanda~Vi\'{e}gas and
            Oriol~Vinyals and
            Pete~Warden and
            Martin~Wattenberg and
            Martin~Wicke and
            Yuan~Yu and
            Xiaoqiang~Zheng},
    year={2015},
}

@article{long_term_dep,
    author={Bengio, Y. and Simard, P. and Frasconi, P.},
    journal={IEEE Transactions on Neural Networks}, 
    title={Learning long-term dependencies with gradient descent is difficult}, 
    year={1994},
    volume={5},
    number={2},
    pages={157-166},
    doi={10.1109/72.279181}
}


@article{lstm,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}
    
@misc{gru_cho,
    title={On the Properties of Neural Machine Translation: Encoder-Decoder Approaches}, 
    author={Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio},
    year={2014},
    eprint={1409.1259},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@inproceedings{attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

