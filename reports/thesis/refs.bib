%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%              References file           %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 
% For the references some tips:
% * Remove all the DOIs and URLs
% * Look up the ISBN number of the work if not present, and add this.
% * If too many authors in one reference, manually remove some and replace with "et. al."

% Outliers, winsorization
@article{winsorization,
    title = {The effects of handling outliers on the performance of bankruptcy prediction models},
    journal = {Socio-Economic Planning Sciences},
    volume = {67},
    pages = {34-42},
    year = {2019},
    issn = {0038-0121},
    doi = {10.1016/j.seps.2018.08.004},
    author = {Tamás Nyitrai and Miklós Virág},
    keywords = {Bankruptcy prediction, Data preprocessing, Winsorizing, Decision trees, CHAID, CART, Neural networks}
}

% For the change of variables formula
@book{murphy,
    added-at = {2017-02-27T11:22:42.000+0100},
    address = {Cambridge, Mass. [u.a.]},
    author = {Murphy, Kevin P.},
    isbn = {9780262018029 0262018020},
    publisher = {MIT Press},
    refid = {904442949},
    timestamp = {2017-02-27T11:22:42.000+0100},
    title = {Machine learning : a probabilistic perspective},
    year = 2013
}

@book{time-series,
    author = {Hyndman, R. J. and Athanasopoulos, G.},
    year = {2018},
    title = {Forecasting: principles and practice, 2nd edition},
    publisher = {OTexts},
    address = {Melbourne, Australia},
    url = {https://otexts.com/fpp2},
    note = {Accessed: 29-08-2023}
}
@article{normalizing_flows,
    doi = {10.1109/tpami.2020.2992934},
    year = 2021,
    month = {nov},
    publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
    volume = {43},
    number = {11},
    pages = {3964--3979},
    author = {Ivan Kobyzev and Simon J.D. Prince and Marcus A. Brubaker},
    title = {Normalizing Flows: An Introduction and Review of Current Methods},
    journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence}
}

@article{yeoJohnson,
    ISSN = {00063444},
    abstract = {We introduce a new power transformation family which is well defined on the whole real line and which is appropriate for reducing skewness and to approximate normality. It has properties similar to those of the Box-Cox transformation for positive variables. The large-sample properties of the transformation are investigated in the contect of a single random sample},
    author = {In-Kwon Yeo and Richard A. Johnson},
    journal = {Biometrika},
    number = {4},
    pages = {954--959},
    publisher = {[Oxford University Press, Biometrika Trust]},
    title = {A New Family of Power Transformations to Improve Normality or Symmetry},
    volume = {87},
    year = {2000}
}

@article{rdain,
    author={Passalis, Nikolaos
            and Kanniainen, Juho
            and Gabbouj, Moncef
            and Iosifidis, Alexandros
            and Tefas, Anastasios},
    title={Forecasting Financial Time Series Using Robust Deep Adaptive Input Normalization},
    journal={Journal of Signal Processing Systems},
    year={2021},
    month={Oct},
    day={01},
    volume={93},
    number={10},
    pages={1235-1251},
    issn={1939-8115},
    doi={10.1007/s11265-020-01624-0},
}

@article{dain,
    title={Deep Adaptive Input Normalization for Time Series Forecasting}, 
    author={Nikolaos Passalis and Anastasios Tefas and Juho Kanniainen and Moncef Gabbouj and Alexandros Iosifidis},
    year={2019},
    eprint={1902.07892},
    journal={arXiv preprint arXiv:1902.07892},
    archivePrefix={arXiv},
    primaryClass={q-fin.CP}
}

@article{bin,
    title={Bilinear Input Normalization for Neural Networks in Financial Forecasting}, 
    author={Dat Thanh Tran and Juho Kanniainen and Moncef Gabbouj and Alexandros Iosifidis},
    year={2021},
    journal={arXiv preprint arXiv:2109.00983},
    archivePrefix={arXiv},
    primaryClass={q-fin.ST}
}

@article{mixture_ct,
    author = {Cao, Zheng and Gao, Xiang and Chang, Yankui and Liu, Gongfa and Pei, Yuanji},
    year = {2023},
    month = {04},
    pages = {e14004},
    title = {Improving synthetic {CT} accuracy by combining the benefits of multiple normalized preprocesses},
    journal = {Journal of applied clinical medical physics},
    doi = {10.1002/acm2.14004}
}

@article{preprocess_origin,
    author = {Sola, J. and Sevilla, Joaquin},
    year = {1997},
    month = {07},
    pages = {1464 - 1468},
    title = {Importance of input data normalization for the application of neural networks to complex industrial problems},
    volume = {44},
    journal = {Nuclear Science, IEEE Transactions on},
    doi = {10.1109/23.589532}
}

@article{singh,
    title = {Investigating the impact of data normalization on classification performance},
    journal = {Applied Soft Computing},
    volume = {97},
    pages = {105524},
    year = {2020},
    issn = {1568-4946},
    doi = {10.1016/j.asoc.2019.105524},
    author = {Dalwinder Singh and Birmohan Singh},
    keywords = {Ant lion optimization, Data normalization, Feature selection, Feature weighting, -NN classifier}
}

@article{nawi,
    title = {The Effect of Data Pre-processing on Optimized Training of Artificial Neural Networks},
    journal = {Procedia Technology},
    volume = {11},
    pages = {32-39},
    year = {2013},
    note = {4th International Conference on Electrical Engineering and Informatics, ICEEI 2013},
    issn = {2212-0173},
    doi = {10.1016/j.protcy.2013.12.159},
    author = {Nazri Mohd Nawi and Walid Hasen Atomi and M.Z. Rehman},
    keywords = {Artificial neural networks, back propagation, gradient descent, gain value, pre-processing data}
}

@inproceedings{stanislav,
    author={Koval, Stanislav I.},
    booktitle={2018 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus)}, 
    title={Data preparation for neural network data analysis}, 
    year={2018},
    pages={898-901},
    doi={10.1109/EIConRus.2018.8317233}
}

@article{boxcox,
    ISSN = {00359246},
    abstract = {In the analysis of data it is often assumed that observations y1, y2, ..., yn are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters θ. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
    author = {G. E. P. Box and D. R. Cox},
    journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
    number = {2},
    pages = {211--252},
    publisher = {[Royal Statistical Society, Wiley]},
    title = {An Analysis of Transformations},
    volume = {26},
    year = {1964}
}

@article{shape,
    ISSN = {00390526, 14679884},
    abstract = {Over the years, various measures of sample skewness and kurtosis have been proposed. Comparisons are made between those measures adopted by well-known statistical computing packages, focusing on bias and mean-squared error for normal samples, and presenting some comparisons from simulation results for non-normal samples.},
    author = {D. N. Joanes and C. A. Gill},
    journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
    number = {1},
    pages = {183--189},
    publisher = {[Royal Statistical Society, Wiley]},
    title = {Comparing Measures of Sample Skewness and Kurtosis},
    urldate = {2023-08-22},
    volume = {47},
    year = {1998}
}

@Inbook{kmeans,
    author="Jin, Xin
    and Han, Jiawei",
    editor="Sammut, Claude
    and Webb, Geoffrey I.",
    title="K-Means Clustering",
    bookTitle="Encyclopedia of Machine Learning",
    year="2010",
    publisher="Springer US",
    address="Boston, MA",
    pages="563--564",
    isbn="978-0-387-30164-8",
    doi="10.1007/978-0-387-30164-8_425",
}


@book{mackay,
    added-at = {2007-05-24T14:43:04.000+0200},
    author = {MacKay, David J. C.},
    publisher = {Copyright Cambridge University Press},
    timestamp = {2007-05-24T14:43:04.000+0200},
    title = {Information Theory, Inference, and Learning Algorithms},
    year = 2003
}

@book{hierarchical_clustering,
    author = {Luca Scrucca and Chris Fraley and T. Brendan Murphy and Adrian E. Raftery},
    title = {Model-Based Clustering, Classification, and Density Estimation Using mclust in {R}},
    year = 2023,
    publisher = {Chapman \& Hall / CRC Press},
    isbn = {9781032234953}
}

@inproceedings{gru,
    title = "Empirical evaluation of gated recurrent neural networks on sequence modeling",
    author = "Junyoung Chung and Caglar Gulcehre and Kyunghyun Cho and Yoshua Bengio",
    year = "2014",
    language = "English (US)",
    booktitle = "NIPS 2014 Workshop on Deep Learning, December 2014",

}

@article{dnn,
    doi = {10.1016/j.neunet.2014.09.003},
    year = 2015,
    month = {jan},
    publisher = {Elsevier {BV}},
    volume = {61},
    pages = {85--117},
    author = {Jürgen Schmidhuber},
    title = {Deep learning in neural networks: An overview},
    journal = {Neural Networks}
}

@inproceedings{backprop,
    author={Hecht-Nielsen},
    booktitle={International 1989 Joint Conference on Neural Networks}, 
    title={Theory of the backpropagation neural network}, 
    year={1989},
    volume={},
    number={},
    pages={593-605 vol.1},
    doi={10.1109/IJCNN.1989.118638}
}

@incollection{pytorch,
    title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
    author = {Adam Paszke et al.},
    booktitle = {Advances in Neural Information Processing Systems 32},
    pages = {8024--8035},
    year = {2019},
    publisher = {Curran Associates, Inc.},
}

@misc{tensorflow,
    title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
    url={https://www.tensorflow.org/},
    note={Software available from tensorflow.org},
    author={
            Mart\'{i}n~Abadi et al.
        },
    year={2015},
}

@article{long_term_dep,
    author={Bengio, Y. and Simard, P. and Frasconi, P.},
    journal={IEEE Transactions on Neural Networks}, 
    title={Learning long-term dependencies with gradient descent is difficult}, 
    year={1994},
    volume={5},
    number={2},
    pages={157-166},
    doi={10.1109/72.279181}
}


@article{lstm,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
}
    
@misc{gru_cho,
    title={On the Properties of Neural Machine Translation: Encoder-Decoder Approaches}, 
    author={Kyunghyun Cho and Bart van Merrienboer and Dzmitry Bahdanau and Yoshua Bengio},
    year={2014},
    eprint={1409.1259},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{attention,
    author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
    publisher = {Curran Associates, Inc.},
    title = {Attention is All you Need},
    volume = {30},
    year = {2017}
}

@misc{amex-data,
  author = {Addison Howard and Aritra and Di Xu and Hossein Vashani and Negin and Sohier Dane},
  year = {2022},
  publisher = {Kaggle},
  title = {{American Express} -- {Default Prediction}},
  howpublished = {\url{https://kaggle.com/competitions/amex-default-prediction}},
  note = {Accessed: 2023-06-09}
}

@misc{amex-starter,
    author = {Chris Deotte},
    year = {2022},
    title = {Time Series {EDA} and {GRU} Starter},
    publisher = {Kaggle},
    howpublished = {\url{https://www.kaggle.com/competitions/amex-default-prediction/discussion/327761}},
    note = {Accessed: 2023-06-14}
}

@article{gini,
    title = {Improving the accuracy of estimates of Gini coefficients},
    journal = {Journal of Econometrics},
    volume = {42},
    number = {1},
    pages = {43-47},
    year = {1989},
    issn = {0304-4076},
    doi = {10.1016/0304-4076(89)90074-2},
    author = {Robert I. Lerman and Shlomo Yitzhaki},
    abstract = {This paper presents a simple method for deriving Gini coefficients that makes full use of the detail from individual records. We develop this procedure for weighted samples of individuals. Next, we compare these precise measures of Gini coefficients with measures based on grouped data. The downward bias from grouped data is small as a percentage of inequality (about 1.5 to 2.0 percent), but rises with the degree of inequality.}
}

@misc{adam,
    title={Adam: A Method for Stochastic Optimization}, 
    author={Diederik P. Kingma and Jimmy Ba},
    year={2017},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{lob-data,
    doi = {10.1002/for.2543},
    year = 2018,
    month = {aug},
    publisher = {Wiley},
    volume = {37},
    number = {8},
    pages = {852--866},
    author = {Adamantios Ntakaris and Martin Magris and Juho Kanniainen and Moncef Gabbouj and Alexandros Iosifidis},
    title = {Benchmark dataset for mid-price forecasting of limit order book data with machine learning methods},
    journal = {Journal of Forecasting}
}

@article{lob_preprocess,
    author = {Alec N. Kercheval and Yuan Zhang},
    title = {Modelling high-frequency limit order book dynamics with support vector machines},
    journal = {Quantitative Finance},
    volume = {15},
    number = {8},
    pages = {1315-1329},
    year  = {2015},
    publisher = {Routledge},
    doi = {10.1080/14697688.2015.1032546},
}

@misc{gould2013limit,
    title={Limit Order Books}, 
    author={Martin D. Gould and Mason A. Porter and Stacy Williams and Mark McDonald and Daniel J. Fenn and Sam D. Howison},
    year={2013},
    eprint={1012.0349},
    archivePrefix={arXiv},
    primaryClass={q-fin.TR}
}

@article{rmsprop,
    author = {Geoffrey Hinton and Tijmen Tieleman},
    title = {Lecture 6.5-rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude.},
    year = {2012},
    journal = {{COURSERA}: Neural Networks for Machine Learning},
    number = {4},
    pages = {26-31},
}

@article{kappa,
    author = {Artstein, Ron and Poesio, Massimo},
    title = {Inter-Coder Agreement for Computational Linguistics},
    year = {2008},
    issue_date = {December 2008},
    publisher = {MIT Press},
    address = {Cambridge, MA, USA},
    volume = {34},
    number = {4},
    issn = {0891-2017},
    doi = {10.1162/coli.07-034-R2},
    journal = {Comput. Linguist.},
    month = {dec},
    pages = {555–596},
    numpages = {42}
}

@article{hassan,
    author={Hassan, Muhammad
            and Hassan, Ishtiaq},
    title={Improving Artificial Neural Network Based Streamflow Forecasting Models through Data Preprocessing},
    journal={KSCE Journal of Civil Engineering},
    year={2021},
    month={Sep},
    day={01},
    volume={25},
    number={9},
    pages={3583-3595},
    issn={1976-3808},
    doi={10.1007/s12205-021-1859-y},
}

@article{outlier_wind,
    title = {Wind power prediction based on outlier correction, ensemble reinforcement learning, and residual correction},
    journal = {Energy},
    volume = {250},
    pages = {123857},
    year = {2022},
    issn = {0360-5442},
    doi = {10.1016/j.energy.2022.123857},
    author = {Shi Yin and Hui Liu},
    keywords = {Wind power prediction, Outlier correction, Ensemble reinforcement learning, Residual correction method},
    abstract = {Wind power prediction contributes to clean energy utilization and grid dispatching. In this study, a wind power prediction model based on outlier correction, ensemble reinforcement learning, and residual correction is proposed. Firstly, the Hampel identifier (HI) is utilized to correct outliers in the original data. Then group method of data handling, echo state network, and extreme learning machine are selected as three base learners to predict the corrected wind power data. And the ensemble reinforcement learning algorithm based on the Q-learning algorithm is utilized to generate optimal ensemble weights to combine the prediction results of three base learners. Finally, the residual correction method (RCM) is applied to revise the prediction results, to obtain the final forecasting results. By comparing the experimental results of the relevant models for four real wind power datasets, it can be known that: (a) The use of both HI and RCM is beneficial to enhance the model's prediction accuracy. (b) The proposed ensemble method based on the Q-learning algorithm has superiority and can achieve smaller prediction errors than three classic ensemble algorithms. (c) The wind power prediction model proposed in this paper has excellent prediction performance and is superior to five commonly used intelligent models.}
}


@book{wasserman,
    added-at = {2012-12-21T15:13:08.000+0100},
    address = {New York},
    author = {Wasserman, Larry},
    isbn = {9781441923226 1441923225},
    keywords = {distribution statistics wasserman},
    publisher = {Springer},
    refid = {668248015},
    timestamp = {2012-12-21T15:13:08.000+0100},
    title = {All of statistics : a concise course in statistical inference},
    year = 2010
}

@inproceedings{brits,
    author = {Cao, Wei and Wang, Dong and Li, Jian and Zhou, Hao and Li, Lei and Li, Yitan},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {BRITS: Bidirectional Recurrent Imputation for Time Series},
    volume = {31},
    year = {2018}
}


@Article{weerakody2023,
    author={Weerakody, Philip B.
            and Wong, Kok Wai
            and Wang, Guanjin},
    title={Cyclic Gate Recurrent Neural Networks for Time Series Data with Missing Values},
    journal={Neural Processing Letters},
    year={2023},
    month={Apr},
    day={01},
    volume={55},
    number={2},
    pages={1527-1554},
    abstract={Gated Recurrent Neural Networks (RNNs) such as LSTM and GRU have been highly effective in handling sequential time series data in recent years. Although Gated RNNs have an inherent ability to learn complex temporal dynamics, there is potential for further enhancement by enabling these deep learning networks to directly use time information to recognise time-dependent patterns in data and identify important segments of time. Synonymous with time series data in real-world applications are missing values, which often reduce a model's ability to perform predictive tasks. Historically, missing values have been handled by simple or complex imputation techniques as well as machine learning models, which manage the missing values in the prediction layers. However, these methods do not attempt to identify the significance of data segments and therefore are susceptible to poor imputation values or model degradation from high missing value rates. This paper develops Cyclic Gate enhanced recurrent neural networks with learnt waveform parameters to automatically identify important data segments within a time series and neglect unimportant segments. By using the proposed networks, the negative impact of missing data on model performance is mitigated through the addition of customised cyclic opening and closing gate operations. Cyclic Gate Recurrent Neural Networks are tested on several sequential time series datasets for classification performance. For long sequence datasets with high rates of missing values, Cyclic Gate enhanced RNN models achieve higher performance metrics than standard gated recurrent neural network models, conventional non-neural network machine learning algorithms and current state of the art RNN cell variants.},
    issn={1573-773X},
    doi={10.1007/s11063-022-10950-2},
}

@misc{lr_demyst,
    title={Demystifying Learning Rate Policies for High Accuracy Training of Deep Neural Networks}, 
    author={Yanzhao Wu and Ling Liu and Juhyun Bae and Ka-Ho Chow and Arun Iyengar and Calton Pu and Wenqi Wei and Lei Yu and Qi Zhang},
    year={2019},
    eprint={1908.06477},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{nearest_psd,
    title = {Computing a nearest symmetric positive semidefinite matrix},
    journal = {Linear Algebra and its Applications},
    volume = {103},
    pages = {103-118},
    year = {1988},
    issn = {0024-3795},
    doi = {10.1016/0024-3795(88)90223-6},
    author = {Nicholas J. Higham},
    abstract = {The nearest symmetric positive semidefinite matrix in the Frobenius norm to an arbitrary real matrix A is shown to be (B + H)/2, where H is the symmetric polar factor of B=(A + AT)/2. In the 2-norm a nearest symmetric positive semidefinite matrix, and its distance δ2(A) from A, are given by a computationally challenging formula due to Halmos. We show how the bisection method can be applied to this formula to compute upper and lower bounds for δ2(A) differing by no more than a given amount. A key ingredient is a stable and efficient test for positive definiteness, based on an attempted Choleski decomposition. For accurate computation of δ2(A) we formulate the problem as one of zero finding and apply a hybrid Newton-bisection algorithm. Some numerical difficulties are discussed and illustrated by example.}
}


@book{num_anal,
    address = {New York},
    author = {Atkinson, Kendall E.},
    edition = {Second},
    isbn = {0471500232},
    priority = {2},
    publisher = {John Wiley \& Sons},
    title = {{An Introduction to Numerical Analysis}},
    year = 1989
}

@article{dnn_survey,
    title = {A survey on deep learning and its applications},
    journal = {Computer Science Review},
    volume = {40},
    pages = {100379},
    year = {2021},
    issn = {1574-0137},
    doi = {10.1016/j.cosrev.2021.100379},
    author = {Shi Dong and Ping Wang and Khushnood Abbas},
    keywords = {Deep learning, Stacked auto encoder, Deep belief networks, Deep Boltzmann machine, Convolutional neural network},
    abstract = {Deep learning, a branch of machine learning, is a frontier for artificial intelligence, aiming to be closer to its primary goal—artificial intelligence. This paper mainly adopts the summary and the induction methods of deep learning. Firstly, it introduces the global development and the current situation of deep learning. Secondly, it describes the structural principle, the characteristics, and some kinds of classic models of deep learning, such as stacked auto encoder, deep belief network, deep Boltzmann machine, and convolutional neural network. Thirdly, it presents the latest developments and applications of deep learning in many fields such as speech processing, computer vision, natural language processing, and medical applications. Finally, it puts forward the problems and the future research directions of deep learning.}
}

@misc{huang2020,
    title={Normalization Techniques in Training DNNs: Methodology, Analysis and Application}, 
    author={Lei Huang and Jie Qin and Yi Zhou and Fan Zhu and Li Liu and Ling Shao},
    year={2020},
    eprint={2009.12836},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{yu2022,
    title={Normalization effects on deep neural networks}, 
    author={Jiahui Yu and Konstantinos Spiliopoulos},
    year={2022},
    eprint={2209.01018},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{batchnorm,
    title={Beyond BatchNorm: Towards a Unified Understanding of Normalization in Deep Learning}, 
    author={Ekdeep Singh Lubana and Robert P. Dick and Hidenori Tanaka},
    year={2021},
    eprint={2106.05956},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
