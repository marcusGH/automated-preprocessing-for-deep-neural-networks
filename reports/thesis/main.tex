\documentclass{statsmsc}

% Vim note: zo, zc and zm to open and close folds, and close all folds

% Commands {{{

\title{Automated selection of preprocessing techniques for deep neural networks}
\author{Marcus Alexander Karmi September}
\CID{01725740}
\supervisor{Francesco Sanna Passino (Imperial College London), \newline Leonie Tabea Goldmann, and Anton Hinel (American Express)}
\date{\today}
%For today's date, use:
%\date{\today}
\logoimg{}


% THIS IS WHERE NEW COMMANDS CAN BE DEFINED
% commands below only used in the proof; otherwise can be deleted
\newcommand{\consta}{a}
\newcommand{\X}{X}
\newcommand{\EE}[1]{ \mathrm{E} [ #1 ] }
\newcommand{\inparenth}[1]{\left( #1 \right)}

% }}}

\begin{document}

%%%%% Heading %%%%%
% Heading {{{

% Generates the Title Page
\maketitle


% Generates plagiarism declaration
\declarationname{Marcus Alexander Karmi September}
\declarationdate{\today}
\declaration


\begin{abstract}
    ABSTRACT GOES HERE
\end{abstract}

\begin{acknowledgements}
    ANY ACKNOWLEDGEMENTS GO HERE
\end{acknowledgements}

{\thispagestyle{plain}
    \tableofcontents
}
% }}}

%%%% Notation %%%%
% Notation {{{
{\chapter*{Notation}\thispagestyle{plain}
    $\bfx, \bfy, \bfz\in\R^d$ are $d$-dimensional vectors

    $\bfX \in \R^{p \times q}$ is a matrix

    $\bfX^{(i)} \in \R^{d \times T}$ denotes the $i$th sample in a dataset, corresponding to a multivariate time-series of length $T$ and dimensionality $d$

    $\bfx^{(i)}_t \in \R^d$ denotes the $d$-dimensional feature vector at timestep $t$ in the $i$th sample in the dataset

    $\bfx^{(i)}_{*,k} \in \R^T$ denotes the $T$-dimensional slice of the $i$th time-series sample, only including the $k$th feature at each timestep.

    $x^{(i)}_{t,j}\in\R$ is the $j$th feature at timestep $t$ in the $i$th sample in the dataset

    $f(x)$ denotes a scalar function $f: \R \rightarrow {\R}$ that maps a single element to a single element

    $\mathbf{f}(\bfx)$ denotes a vector function $\mathbf{f}:\R^d \rightarrow {\R^d}$ applied to a $d$-dimensional vector

    $\oplus, \ominus, \odot$, and $\oslash$ denotes addition, subtraction, multiplication, and division, respectively, applied element-wise between two $d$-dimensional vectors. For example, $\bfx \oslash \bfy$.

    $\mathcal{D}$ denotes a dataset

    $\mathcal{B}$ denotes a set of indices from a dataset, referred to as a \textit{batch}

    $\mathbf{J}_{\bfZ \rightarrow {\bfY}}$ denotes the Jacobian matrix of a function $\mathbf{f} : \R^d \rightarrow {\R^d}$ that maps samples $\bfz \sim \bfZ$ to samples $\bfy \sim \bfY$, that is,
    $\bfy=\mathbf{f}(\bfz)$.

    $\mathbb{I}(p)$ denotes the indicator function and takes value 1 when condition $p$ is true,
    0 otherwise.

    $\#A$ and $|A|$ both refer to the cardinality of a set $A$, that is, the number of elements contained in $A$.

% }}}

%%%% Abbreviations and acronyms %%%%
% Abbreviations {{{
{\chapter*{Abbreviations}\thispagestyle{plain}
    \begin{acronym}[TDMA]
        \acro{DAIN}{Deep Adaptive Input Normalization}
        \acro{RDAIN}{Robust Deep Adaptive Input Normalization}
        \acro{EDAIN}{Extended Deep Adaptive Input Normalization}
        \acro{EDAIN-KL}{Extended Deep Adaptive Input Normalization, optimised with Kullback–Leibler divergence}
        \acro{BIN}{Bilinear Input Normalization}
        \acro{pdf}{probability density function}
        \acro{KL-divergence}{Kullbeck-Leibler divergence}
        \acro{PREPMIX-CAPS}{Preprocessing Mixture, optimised with Clustering and Parallel Search}
        \acro{API}{Application Programming Interface}
        \acro{GPU}{Graphics Processing Unit}
        \acro{RNN}{Recurrent Neural Network}
        \acro{GRU}{Gated recurrent unit}
        \acro{LSTM}{Long short-term memory}
        \acro{ReLU}{Rectified Linear Unit}
        \acro{LOB}{limit order book}
    \end{acronym}
}

% VERY IMPORTANT
% This command switches from Roman to Arabic numbering for main part of thesis
\mainmatter

% }}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction} %%%%     Introduction      %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction {{{

The introduction section goes here\footnote{Tip: write this section last.}.

% TODO: in addition to methods as part of **MAIN CONTRIBUTIONS**, also present that additional
%       contribution is some novel insight on local- vs. global-ware normalization.

% }}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background} %%%%        Background       %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Introduction
% Introduction {{{

In this project, we make extensive use of deep learning method, especially sequence models as
both of the real-world datasets and synthetic dataset we will be working with all contain
multivariate time-series.  Therefore, we start this chapter off with a background on deep
learning, building up to sequence models.
Another big part of the project is investigating the effect of different
preprocessing techniques applied to the data before it is passed onto the deep neural network.
As such, this chapter also covers the most commonly used \textit{static preprocessing methods} used
in the literature. Additionally, we will look at the current state of the art when it comes
to preprocessing multivariate time-series data, which includes three
\textit{adaptive preprocessing methods} whose unknown parameters are trained in the same
fashion as neural networks.

% }}}

\section{Deep learning}% Deep learning
% Methods: Deep learning {{{
\label{sec:Deep learning}

Deep learning has played a significant role in improving predictive performance in many fields,
ranging from financial forecasting \citep{dain,rdain,bin} to machine
translation \citep{gru_cho,attention} to computed tomography analysis
\citep{mixture_ct}. In this section, we provide a brief overview of how neural networks work and
how they are trained using a dataset. We do this by first describing the standard \textit{linear},
or feedforward-type, neural network. Then we move onto describing how neural networks are trained,
including descriptions of loss functions, stochastic gradient descent, and backpropagation. We will
also cover extensions to the standard training framework, including early stoppers, learning
rate schedulers and optimizers with adaptive learning rates.

\subsection{The standard linear neural network}%
\label{sub:The standard linear neural network}

The standard neural network consists of $L$ \textit{linear} layers, each containing
$n_1,n_2,\dots,n_L$ perceptrons \citep{dnn}. An input sample $\bfx \in \R^d$ can be fed
through the neural network, producing \textit{post-activations} at each layer, denotes
$\bfz^{(1)}, \dots, \bfz^{(L)}$. The post-activations are produced through weighted
connections between each neuron and all the neurons in the previous layer. If we let
$\bfz^{(0)}=\bfx \in \R^d$ denote the input and let $n_0=d$, we have for $\ell=1,\dots,L$
\begin{equation}\label{eq:nn_update}
    z_j^{(\ell)} = \sigma \left(\left[ \boldsymbol{W}^{(\ell)} \bfz^{(\ell-1)} + \boldsymbol{b}^{(\ell)} \right]_j \right),\qquad j=1,\dots,n_\ell,
\end{equation}
where $\boldsymbol{W}^{(\ell)} \in \R^{n_{\ell} \times n_{\ell-1}}$ is the \textit{weight matrix},
$\boldsymbol{b}^{(\ell)} \in \R^{n_{\ell}}$ is a \textit{bias} term and
$\sigma : \R \rightarrow {\R}$ is some deterministic \textit{activation function}.
To get the output of the neural network, we iteratively calculate the post-activations
$\bfz^{(1)},\bfz^{(2)},\dots$ until we get to $\bfz^{(L)}$, which we denote as the output
$\hat{\bfy}$. The dimensionality of $\hat{\bfy}=\bfz^{(L)} \in \R^{n_L}$
depends on the problem one wants
to apply the neural network to. For example, if doing regression, one typically sets
$n_L=1$, giving $\hat{\bfy} \in \R$. If one wants to classify some inputs in one of three classes,
one could set $n_L=3$ and interpret $\hat{\bfy} \in \R^3$ as unnormalized log-probabilities of
the sample $\bfx \in \R^d$ belonging to each of the 3 classes.

\subsection{Training a neural network}%
\label{sub:Training a neural network}

During training of the neural network, we want to optimise the \textit{unknown
parameters} $\bm{\theta}=(\mathbf{W}, \mathbf{b})$, where
$\mathbf{W}=\left(\boldsymbol{W}^{(1)},\dots,\boldsymbol{W}^{(L)}\right)$ and
$\mathbf{b}=\left(\boldsymbol{b}^{(1)},\dots,\boldsymbol{b}^{(L)}\right)$, in
order to minimize some \textit{criterion} $\mathcal{L}: \R^{n_L} \times \R^{n_L}
\rightarrow {\R}$. Some common criteria are the mean squared error and the
cross-entropy loss function.
More concretely, given a \textit{training dataset }
$\mathcal{D}=\{(\bfx^{(i)}, \bfy^{(i)})\}_{i=1,2,\dots,N}$ of inputs
$\bfx^{(i)} \in \R^d$ and \textit{targets} $\bfy \in \R^{n_L}$, we want to find
\begin{equation}\label{eq:nn_loss}
    \widehat{\bm\theta}= \argmin_{\bm\theta}
    \frac{1}{N}  \sum^{N}_{i=1} \mathcal{L}(\hat{\bfy}^{(i)}, \bfy^{(i)}),
\end{equation}
where as evident from \cref{eq:nn_update}, $\hat{\bfy}^{(i)}$ is a function of
$\bfx^{(i)}$ and the unknown parameters $\bm\theta$.
In most situations, there is no analytic solution to \cref{eq:nn_loss}, so
the parameters $\bm\theta$ are optimised through \textit{stochastic gradient descent},
where the gradients are computed with \textit{backpropagation}.
The backpropagation algorithm is an efficient method of computing the gradients
$\frac{\partial }{\partial \bm\theta} \mathcal{L}(\hat{\bfy}^{(i)}, \bfy^{(i)}) $
using the chain-rule. A more comprehensive description of the algorithm can be
found in \citep{backprop}.
After computing the gradients, the weights and biases $\bm\theta$ are updated through
stochastic gradient descent, which involves estimating
the full gradient using only a \textit{sample batch} of the training data,
$\mathcal{B}=\{i_1,i_2,\dots,i_B\}$, where $B$ is the \textit{batch-size} and
$1\leq i_1,i_2,\dots,i_B \leq N$ are indices into the training dataset $\mathcal{D}$.
If let $J(\bm\theta)$ denote the \textit{objective} to minimize in \cref{eq:nn_loss}, that is
\begin{equation}
    J(\bm\theta)=\frac{1}{N} \sum^{N}_{i=1} \mathcal{L}(\hat{\bfy}^{(i)}, {\bfy}^{(i)}),
\end{equation}
then we estimate its gradient with
\begin{equation}
    \widehat{\nabla_{\bm\theta} J(\bm\theta)} = \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}
    \nabla_{\bm\theta} \mathcal{L}(\hat{\bfy}^{(i)}, {\bfy}^{(i)}).
\end{equation}
After computing this estimate, we update the unknown parameters by setting a \textit{stepsize}
$\eta \in \R$ and performing the parameter update:
\begin{equation}\label{eq:sadaosidhaoisdh89}
    \bm\theta \leftarrow \bm\theta - \eta \widehat{\nabla_{\bm\theta} J(\bm\theta)}.
\end{equation}
This is usually done once for each of the batches
$\mathcal{B}_1,\mathcal{B}_2,\dots, \mathcal{B}_{\lceil N / B \rceil}$, where
the batches are a partition of the indices of the training dataset
$\mathcal{D}$. This sequence of $\lceil N / B \rceil$ parameter updates, once for each batch, is
referred to as one \textit{training epoch}.
When training a neural network, one usually optimise the parameters by repeating this process
for several epochs, for example 20 epochs.


% Early stopping paragraph
One way of improving generalization performance, that is, how well the model performs on data
not present in the training data, is to use \textit{early stopping} when training the neural
network. To do this, the training data $\mathcal{D}$ is split into a \textit{training set}
$\mathcal{D}_{\textrm{train}}$ and validation set $\mathcal{D}_{\textrm{val}}$, where only
$\mathcal{D}_{\textrm{train}}$ is used for the parameter updates.
Then, after each epoch, the average value of the criterion $\mathcal{L}(\cdot,\cdot)$ is computed on
the validation set, giving the \textit{validation loss}. If we start seeing the validation loss
increasing at some point, training is terminated. During neural network training, the loss
might jump around a lot during convergence, so one typically specifies the
\textit{patience} $\in\mathbb{N}$
for the early stopper. After each epoch, one also keeps track of the lowest validation loss achieved
so far, and if the model trains for a patience number of epochs, without achieving a validation
loss lower than the lowest recorded validation loss so far, the training is terminated.

% Learning rate scheduling paragraph
Certain neural network architectures might also not efficiently convergence if the learning
rate $\eta \in \R$ is held fixed, which can be solved by using a \textit{learning rate scheduler}.
A learning rate scheduler, $\eta: \mathbb{N} \rightarrow {\R}$ is usually a
monotonically non-increasing function that maps the current epoch number $t \in \mathbb{N}$
to the learning rate $\eta \in \R$ to use when updating the parameters at epoch $t$. With
a learning rate scheduler, the parameter update in \cref{eq:sadaosidhaoisdh89} can be reformulated
as
\begin{equation}
    \bm\theta^{(t+1)} \leftarrow \bm\theta^{(t)} - \eta(t) \widehat{\nabla_{\bm\theta} J\left(\bm\theta^{(t)}\right)},
\end{equation}
where $\bm\theta^{(t)}$ denotes the parameter values at epoch $t \in \mathbb{N}$.

% Brief background on GPUs
During both the \textit{forward passes}, as described by \cref{eq:nn_update}, and the
\textit{backwards passes}, where the gradients are computed, a lot of operations that can be
formulated through matrix multiplications are performed \citep{backprop}.
This can efficiently be parallelised on a \acp{GPU}, so deep learning is typically done using
libraries built to execute code on the computer's \ac{GPU}, as this reduces computation time
during both training and inference. Popular Python libraries for deep learning leveraging
\acp{GPU} to efficiently speed up computation include PyTorch \citep{pytorch} and TensorFlow \citep{tensorflow}.

\subsection{Sequence models}%
\label{sub:Sequence models}

In the previous section, we talked about conventional feedforward--or linear--neural networks,
and how these take input samples on the form $\bfx \in \R^d$. Sequence models such as
\acp{RNN} extend the linear neural networks and can handle variable-length sequences
$\bfX \in \R^{d \times T}$, where $T \in \mathbb{N}$ is the sequence length. We might alternatively
denote these sequences with $\bfxvec=(\bfx_1,\bfx_2,\dots,\bfx_T)$, where $\bfx_i\in\R^d$.
Traditional \acp{RNN} do this by iteratively updating its \textit{recurrent hidden state} $\mathbf{h}_t \in
\R^{n_{\textrm{hidden dim} }}$
\begin{equation}\label{eq:rnn_trad}
    \mathbf{h}_t=
    \left\{
        \begin{array}{ll}
            0, & t=0 \\
            \sigma\left(\mathbf{W}\bfx_t + \mathbf{U}\mathbf{h}_{t-1} \right), & \textrm{otherwise}
        \end{array}
    \right.,
\end{equation}
where $\sigma(\cdot): \R \rightarrow {\R}$ is a smooth non-linear activation function,
and $\mathbf{W}$ and $\mathbf{U}$ are the unknown weights \citep{gru}.
The output of the \ac{RNN} is then the sequence
$\vec{\bfh}=\left(\bfh_1,\bfh_2,\dots,\bfh_T \right)$, which can subsequently be fed into other
neural network components depending on the task to be solved. For example, if
classifying sequences, the last element of $\vec{\bfh}$ can be fed into a conventional
linear neural network that outputs a vector of unnormalized log-probabilities for each of
the classes. On the other hand, if the task is to predict the next \textit{token} in the
sequence, the vectors $\bfh_1,\bfh_2,\dots,\bfh_T$ can separately be fed into a feedforward
neural network that produces a probability distribution over the set of all possible next tokens.

Unfortunately, the traditional \acp{RNN} presented in \cref{eq:rnn_trad} cannot
capture long-term dependencies in the input sequences very well
\citep{long_term_dep}. Therefore, more sophisticated recurrent update equations
than the one in \cref{eq:rnn_trad} have been proposed, such as the \ac{LSTM}
cell \citep{lstm} and the \ac{GRU} \citep{gru_cho}.
In later years, even more sophisticated model architectures for handling sequence data with
long-term dependencies, such as the transformer \citep{attention}, have been proposed.

% }}}

\section{Data preprocessing}% Data preprocessing
% Methods: Data preprocessing {{{
\label{sec:Data preprocessing}

Data preprocessing in machine learning has been studied as early
as \citeyear{preprocess_origin} by for instance \citeauthor{preprocess_origin}.
Moreover, several works study its effects on neural networks \citep{stanislav, nawi, singh, hassan}.
In this section, we first cover \textit{static preprocessing methods}, which
are transformations where the parameters are simple statistics that can be
computed directly
\marginpar{\scriptsize ``Simple statistics?'' correct term?}
from the dataset, such as the mean, minimum, standard deviation etc.
We cover the methods most commonly such in the literature, such as min-max
normalization, Z-score scaling, decimal scaling, Box-Cox transformation,
and winsorization \citep{stanislav, nawi, singh, hassan, winsorization}.
We will also look at two different ways all these methods can be applied to multivariate
time-series data.
%
After this, we move onto describing three state of the art adaptive
preprocessing methods from work by \cite{dain,rdain,bin}.
In the adaptive preprocessing techniques, the preprocessing layers also have
unknown parameters, but instead of setting them equal to simple statistics of
the dataset, we iteratively tune them through gradient descent while training the
neural network. Additionally, these three layers all
perform a \textit{local-aware} normalization, meaning they also consider
summary statistics computed for each individual time-series sample when
deciding how to normalize it.

% TODO: notation not consistent when indexing dimension and time
\marginpar{FIXME:
    \scriptsize Inconsistent notation when indexing into $\bfX^{(i)} \in \R^{d\times T}$.
    Should you do $x_{j,t}$ or $x_{t,j}$, and then what about $x_{*,t}$?}

% Static distribution transformations
\subsection{Static distribution transformations}% {{{
\label{sub:Static distribution transformations}

In this subsection, we are working with $N$ samples, each of $d$ dimensions, which we
denote as $\mathcal{D}=\{\bfx^{(i)} \}_{i=1,2,\dots,N}$. When talking about a general operation
on a sample $\bfx^{(i)}\in\R^d$, as in \cref{eq:pp1,eq:pp2,eq:pp3,eq:pp4,eq:pp5,eq:pp6,eq:pp7},
we will drop the sample index and just use the notation
$\tilde{\bfx}=\left(\tilde{x}_1,\tilde{x}_2,\dots,\tilde{x}_d \right)
=(f_1(x_1), f_2(x_2),\dots,f_d(x_d))$ to denote applying some transformation $\mathbf{f}(\cdot)$ to
$\bfx$, element-wise, but with different parameters for each element.
Moreover, for $j=1,2,\dots,d$, we let
\begin{align}
    x_j^{(min)}=\min_i x^{(i)}_j,  \qquad\qquad&\quad
    x_j^{(max)}=\max x^{(i)}_j, \nonumber\\
    \mu_j = \frac{1}{N} \sum^{N}_{i=1} x^{(i)}_j, \quad
    \textrm{ and }&\quad
    \sigma_j = \sqrt{\frac{1}{N} \sum^{N}_{i=1} \left( x^{(i)}_j - \mu_j\right)^2}.
\end{align}

With notation out of the way, we now proceed with describing some of the most common
static preprocessing techniques.  The Min-Max transformation can be
used to transform the data to the range $[0, 1]$ by performing the following operation:
\begin{equation}\label{eq:pp1}
    \tilde{x}_j = \frac{x_j-x_j^{(min)}}{x_j^{(max)}-x_j^{(min)}} .
\end{equation}
It can also be modified to transform the data to the range $[-1,+1]$ with
\begin{equation}\label{eq:pp2}
    \tilde{x}_j = 2\cdot\frac{x_j-x_j^{(min)}}{x_j^{(max)}-x_j^{(min)}}-1.
\end{equation}
Standard scaling, also known as Z-score scaling, is also a common preprocessing technique and
is done with
\begin{equation}\label{eq:pp3}
    \tilde{x}_j=\frac{x_j-\mu_j}{\sigma_j}.
\end{equation}
One can also apply an activation function after performing Z-score scaling \citep{nawi},
giving
\begin{equation}\label{eq:pp4}
    \tilde{x}_j=f\left(\frac{x_j-\mu_j}{\sigma_j}\right).
\end{equation}
For example, \citeauthor{mixture_ct} use $f=\tanh$ to constrain the data into domain $[-1,+1]$.
Another option is decimal scaling, which is the operation
\begin{equation}\label{eq:pp5}
    \tilde{x}_j=\frac{x_j}{10^{a_j}}, \quad \textrm{ where } a_j
    \textrm{ is the smallest integer that satisfies }
        \left|\frac{x_j^{(max)}}{10^{a_j}}  \right|<1.
\end{equation}
We also have the Box-Cox transformation, proposed by \citeauthor{boxcox}:
\begin{equation}\label{eq:pp6}
    \tilde{x}_j=\left\{
        \begin{array}{ll}
            \frac{x_j^\lambda-1}{\lambda}, & \textrm{ if } \lambda \neq 0 \\
            \log(x_j), & \textrm{ if } \lambda=0
        \end{array}
    \right.,
\end{equation}
which works for positive $x_j$ and is a power-transformation that can reduce the skewness of
a distribution.  If the data has outliers, a transformation for reducing the effects of these
is what is called \textit{winsorization}, or clipping,
where the transformation is
\begin{equation}\label{eq:pp7}
    \tilde{x}_j=\max\left\{q_j^{(\alpha/2)},\;\min\left(q_j^{(1-\alpha/2)},\; x_j\right)\right\},
\end{equation}
where $q_j^{(\beta)}$ denotes the $\beta$th quantile along the $j$th dimension of the dataset
$\mathcal{D}$. The effect of this preprocessing method for handling outliers has been
studied by \cite{winsorization}.

So far, we have only considered $d$-dimensional datasets, but when working with multivariate
time-series, there is also a temporal dimension $T$, giving samples on the form
$\bfX \in \R^{d\times T}$. There are two approaches to applying the
transformations in \cref{eq:pp1,eq:pp2,eq:pp3,eq:pp4,eq:pp5,eq:pp6,eq:pp7}
to such datasets. Say we are working with a transformation $\mathbf{f}: \R^d \rightarrow {\R^d}$, where the
parameters such as $\bm\mu$, $\bm\sigma$, $\bfx^{(min)}$, and $\bfx^{(max)}$ have been learned
from a set of samples $\mathcal{D}$. The first approach, which I will refer to as
\textit{preprocessing across time}, involves merging the time-axis with the sample-axis, giving
an augmented dataset $\mathcal{D}'=\{\bfx^{(i\cdot T +t)}\}_{i=1,2,\dots,N,t=1,2,\dots,T}$
containing $N \cdot T$ samples, each of dimensionality $d$. This dataset $\mathcal{D}'$ is then
used to estimate the transformation parameters, and to transform each sample, we do
$\tilde{x}_{j,t}=f_j(x_{j,t})$ regardless of what the value of $t$ is.

% TODO: this might not make much sense when read...
In the second approach, which will be referred to as
\textit{preprocessing with time- and dimension-axis}, we do not augment the dataset. Instead,
we merge the time-axis and dimension-axis, and learn the transformation parameters for
each of the $d \cdot T$ ``new features''. That is, we let
$\mathcal{D}''=\left\{\left[\bfx^{(i)}_{*,1} \; \bfx^{(i)}_{*,2} \; \cdots \; \bfx^{(i)}_{*,T} \right]^\top \right\}_{i=1,2,\dots,N}$ be our new dataset of $N$ samples of $d\cdot T$-dimensional samples, and
use this to find the transformation parameters. The transformation applied to the value
$\bfx^{(i)}_{j,t}$ then depends on both $j$ and $t$.

% }}}

% Adaptive distribution transformation
\subsection{Adaptive distribution transformations}% {{{
\label{sub:Adaptive distribution transformations}

% TODO: write adaptive part next now...
% Points to get across:
% * In adaptive preprocessing methods, have unknown parameters that can't be directly inferred from
%   simple statistics computed on the training dataset
% TODO: introduction here that mentions what I mean by "adaptive" and how
%       different from the "static" preprocessing techniques

% TODO: this does not read very well! FIX THIS!
We now move onto the adaptive preprocessing techniques. The static preprocessing techniques have unknown parameters that can be
fully represented by summary statistics, for instance the mean and standard deviation of the training dataset. The adaptive
preprocessing technique, however, have unknown parameters that need to be trained for a specific problem and neural network in mind.
That is, they are inferred in an end-to-end fashion. That way, the preprocessing layers can \textit{adapt} to normalize the data
in whatever fashion is most suitable for the particular task and model being used.

% DAIN layer subsection
\subsubsection{DAIN}% {{{
\label{ssub:DAIN}


% Talking about DAIN:
% * Main motivation is being able to handle nonstationarity of time-series,
%   that is mean and or variance might be a function of time
% * More on this..
% * Three sublayers:
%   * shift the data, which centres the the data in the feature space
%   * scaling, increase or reduce variances of samples, depending on what's most appopriate
%   * third layer performs gating, which is a non-linear operation that is able
%     to supress irrelvant features, that is, perform some notion of feature
%     selection
% * adaptive in the sense that the parameters can be trained on input data
% * local-aware in the sense that each normalization operation depends on the
%   current sample through summary aggregators. The normalization "adapts" to the
%   local distribution
% * Designed around application to financial forecasting problems, where dataset highly multimodal
% * local-aware normalization, which transforms all time-series into common representation space

% To produce the diagram, run
%     pdfcrop --margins '-15 -30 0 -560' dain_paper_page2.pdf dain_diagram.pdf
% on the second page of a PDF copy of the DAIN paper by Passalis et al.
\begin{figure}
    \begin{center}
        \includegraphics[width=\textwidth]{diagrams/dain_diagram.pdf}
    \end{center}
    \caption{Architecture of the \acf{DAIN} layer, proposed by \citeauthor{dain}. The diagram
    is taken from page 2 of \citep{dain}.}
    \label{fig:dain-arch}
\end{figure}

The \ac{DAIN} layer is one of the earliest preprocessing methods that can
handle highly multimodal and non-stationary multivariate time-series in an adaptive,
end-to-end fashion \cite{dain}.
This layer was designed for application on financial forecasting tasks, where highly multi-modal
time-series are common. Additionally, these time-series are often non-stationary, that is, the
mean and variance of the data no not remain constant across time.
Both of these aspects makes Z-score normalization unsuitable as the statistics can differ
from one timestep to another, and Z-score scaling is not suitable on multimodal distributions.
The \ac{DAIN} layer handles these issues through three adaptive sublayers that all depend
on both summary statistics of the current sample being normalized, as well unknown transformation
parameters that can be trained for the specific dataset being used.
As we see in \cref{fig:dain-arch}, the first sublayer is an adaptive shift layer
that centres the data. The second sublayer is an adaptive scaling layer that can increase or
reduce the variance of each sample. The third sublayer is a non-linear gating operation
that can suppress irrelevant features, that is, perform feature selection.

Before delving deeper into how exactly the sublayers operate, we consider an illustrative
example of what might cause a high number of modes in financial data and why this might
be problematic for deep sequence models. In
\cite{dain}, the authors provide an illustrative example of this:
\begin{quote}
    ``\textit{[A]ssume two tightly
    connected companies with very different stock prices, e.g., 1\$
    and 100\$ respectively. Even though the price movements can
    be very similar for these two stocks, the trained forecasting
    models will only observe very small variations around two
    very distant modes (if the raw time series are fed to the model).}''
\end{quote}
By normalizing each time-series in what I will refer to as a
\textit{local-aware} fashion, that is, make the normalization also depend on summary statistics
based on the particular sample being normalized,
the amount of shifting and scaling can depend on what particular mode in the dataset the sample
came from, and this information can be discarded. This allows transforming all the samples into
a common more unimodal representation space, despite the input data being highly multimodal.

We look at an overview of the \ac{DAIN} architecture, of which an overview is shown
in \cref{fig:dain-arch}. The unknown parameters are the weight matrices
$\mathbf{W}_a$, $\mathbf{W}_b$, $\mathbf{W}_c \in \R^{d \times d}$, and the bias
term $\mathbf{d} \in \R^d$, and are used
for shift, scale, and gating layer, respectively. The adaptive shift layer and
adaptive scale layer, together, perform the operation
\begin{equation}\label{eq:dain_tild}
    \tilde{\bfx}_t^{(i)}=\left(
        \bfx_t^{(i)}-\mathbf{W}_a \mathbf{a}^{(i)}
    \right) \oslash \mathbf{W}_b \mathbf{b}^{(i)},
\end{equation}
where $\mathbf{a}^{(i)}$ and $\mathbf{b}^{(i)}$ are summary statistics that are computed for
the $i$th sample as follows:
\begin{align}
    \mathbf{a}^{(i)}& =\frac{1}{T} \sum^{T}_{t=1} \bfx_t^{(i)}, \label{eq:dain_a}\\
    b_k^{(i)}&=\sqrt{
    \frac{1}{T} \sum^{T}_{t=1} \left(x^{(i)}_{t,k} - \left[\mathbf{W}_a  \mathbf{a}^{(i)}\right]_k \right)^2},
    \qquad k=1,2,\dots,d. \label{eq:dain_b}
\end{align}
The third sublayer, the gating layer, performs the operation
\begin{equation}
    \tilde{\tilde{\bfx}}_t^{(i)}=\tilde{\bfx}^{(i)} \odot \textrm{sigm}\left( \mathbf{W}_c \mathbf{c}^{(i)} + \mathbf{d}\right),
\end{equation}
where $\textrm{sigm}(\cdot)$ denotes the sigmoid function, defined as
$\textrm{sigm}(x)=1/(1+e^{-x})$, and $\mathbf{c}^{(i)}$ is the third summary statistic, computed
with
\begin{equation}
    \mathbf{c}^{(i)}=\frac{1}{T} \sum^{T}_{t=1} \tilde{\bfx}_t^{(i)}.
\end{equation}

% }}}

% RDAIN layer subsection
\subsubsection{RDAIN}% {{{
\label{ssub:RDAIN}

% RDAIN:
% * By same author as DAIN paper, extends it with a residual connection
% * Additionally, introduce bias terms
% * Show diagram
A few years after \citeauthor{dain} proposed the \ac{DAIN} layer in \citeyear{dain}, they
improved on
this layer with the \ac{RDAIN} architecture \citep{rdain}. This adaptive preprocessing layer
extends \ac{DAIN} with a local-aware residual connection that skips the adaptive shift and
scale sublayers. Additionally, the adaptive shift and scale sublayers in \ac{RDAIN} also
include a trainable bias term, not just a weight matrix.

% \begin{equation}
%     \bm\alpha(\bfx)=\mathbf{W}_\alpha \mathbf{s}_\alpha(\bfx) + \mathbf{b}_\alpha \in \R^d
% \end{equation}
% \begin{equation}
%     \bm\beta(\bfx)=\left(\mathbf{W}_{\beta} \mathbf{s}(\bfx)+\mathbf{b}_\beta \right)^{-1}
% \end{equation}
% \begin{equation}
%     \gamma(\bfx)=\textrm{sigm}\left(\mathbf{W}_\gamma \mathbf{s}_\gamma(\bfx)+\mathbf{b}_\gamma \right)
% \end{equation}


% }}}

% BIN layer
\subsubsection{BiN}% {{{
\label{ssub:BiN}


% BIN:
% * Also uses "sample-level statistics" to adapt to the local distribution
% * Performs similar operation as DAIN layer, but does it across both time and dimension axis,
%   then it combines the two results with a linear combination
% * Using the same notation is with DAIN, the operation is
% * TODO: equations for the operations, such that easy to understand ....
%   * Do this while reading the BIN paper

The third adaptive preprocessing method is the \ac{BIN} layer, initially
presented by \cite{bin}. It has a similar architecture to \ac{DAIN}, but drops the
third gating layer. Additionally, while the \ac{DAIN} only does an adaptive shift and scale layer
based on a summary representation found from reducing along the time-axis, the \ac{BIN} layer
does a similar operation twice, once across time, and once across dimension axis, then returns
a linear combination of these two normalized time-series as the final output.

We now look at the adaptive scale- and shift sublayers of the \ac{BIN} architecture.
Recall that one sample is a multivariate time-series on the form $\bfX^{(i)} \in \R^{d \times T}$.
Like the \ac{DAIN} layer in \cref{eq:dain_a,eq:dain_b}, the \ac{BIN} layer also
compute two summary representations of each sample along the time-\textit{column}:
\begin{align}
    \overline{\mathbf{c}}^{(i)}& =\frac{1}{T} \sum^{T}_{t=1} \bfx_t^{(i)} \in \R^d, \\
    \bm\sigma_c^{(i)}&=\sqrt{
    \frac{1}{T} \sum^{T}_{t=1} \left(\bfx^{(i)}_{t} - \overline{\mathbf{c}}^{(i)} \right)
    \odot \left(\bfx^{(i)}_{t} - \overline{\mathbf{c}}^{(i)} \right)
} \in \R^d.
\end{align}
These summary representation are then used together with unknown parameters
$\bm\gamma_c \in \R^d$ and $\bm\beta_c \in \R^d$ to produce a sample that is normalized across
the time \textit{column}:
\begin{equation}
    \tilde{\bfx}^{(i)}_t= \bm\gamma_c \odot \left\{
        \left(\mathbf{x}^{(i)}_t - \overline{\mathbf{c}}^{(i)}\right)\oslash \bm\sigma_c^{(i)}
    \right\}+\bm\beta_c, \qquad \forall t=1,2,\dots,T.
\end{equation}
Notice how the order of operation multiplication of the unknown parameters differ from what
\ac{DAIN} does in \cref{eq:dain_tild}. Similarly, along the dimension \textit{row}, we compute
the summary representations:
\begin{align}
    \overline{\mathbf{r}}^{(i)}& =\frac{1}{d} \sum^{d}_{k=1} \bfx_{*,k}^{(i)} \in \R^T, \\
    \bm\sigma_r^{(i)}&=\sqrt{
        \frac{1}{d} \sum^{d}_{k=1} \left(\bfx^{(i)}_{*,k} - \overline{\mathbf{r}}^{(i)} \right)
        \odot \left(\bfx^{(i)}_{*,k} - \overline{\mathbf{r}}^{(i)} \right)
    } \in \R^T.
\end{align}
The row-summaries are then used together with new unknown parameters
$\bm\gamma_r \in \R^T$ and $\bm\beta_r \in \R^T$ to produce
a sample that is normalized across the dimension \textit{row}:
\begin{equation}
    \tilde{\tilde{\bfx}}^{(i)}_{*,k}= \bm\gamma_r \odot \left\{
        \left(\mathbf{x}^{(i)}_{*,k} - \overline{\mathbf{r}}^{(i)}\right)\oslash \bm\sigma_r^{(i)}
    \right\}+\bm\beta_r, \qquad \forall k=1,2,\dots,d.
\end{equation}
The output of the \ac{BIN} layers is then a linear combination of these two normalized samples:
\begin{equation}
    % TODO: add some other symbol above the first x
    \stackrel{\mathbat}{x}^{(i)}_{t,k} = \lambda_c \tilde{x}^{(i)}_{t,k}
    + \lambda_r \tilde{\tilde{x}}^{(i)}_{t,k},
\end{equation}
where $\lambda_c \in \R$ and $\lambda_r$ are two learnable scalars that allows the \ac{BIN} layer
to learn how much to weight each of the normalization methods.


% }}}


% END adaptive layer }}}


% END data preprocessing }}}

\section{Conclusion}% Conclusion
\label{sec:Conclusion}% {{{

% Deep learning:
% * Linear layer, works by multiplying post-activations from previous layer with
%   weight matrix and bias term, then passed through activation function,
% * training NN using stochastic gradient descent, where pass one batch of
%   samples at the time and look at the average gradient of the loss with respect
%   to unknown parameters, 
% * iteratively update the parameter values based on this gradient with a learning rate
% * also covered early stoppers for imrpvoing generalization performance, and
%   lr scheduler for more stable convergence
% * Then moved onto sequence model and described the basic RNN cell as well as the GRU cell
In this chapter, we provided an overview of deep learning and various data preprocessing techniques
suitable for multivariate time-series. We looked at the linear neural network layer, which
produces pre-activations that are weighted sums of the previous post-activations and bias terms.
These are then passed through a non-linear activation function to produce the next vector of
post-activations. We also looked at training the neural network with stochastic gradient descent,
where the gradients of the loss with respect to the unknown parameters are estimated on a batch of
data using backpropagation. The gradient estimates are then used to iteratively update the model
parameters until a certain number of epochs have elapsed. We also looked at using early stopping,
learning rate scheduling and adaptive optimizer to make the training process more efficient and
stable, as well as improving generalization performance. Finally, we covered some common building
blocks used in creating deep sequence models.

% * data preprocessing.Covered static distribution transformations such as min-max, Z-score
%   scaling, Z-score scaling with tanh activation function, decimal scaling,
%   the Box-Cox transformation for handling skewed data, and winsorization for
%   handling outliers
% * computed using statistics to estimate the mean, standard deviation, minimum, maximum, etc.
% * then moved onto adaptive preprocessing techniques, such as the DAIN, RDAIN, BIN
% * DAIN consists of an adaptive scale and shift layer, followed by an adaptive
%   gating layer used for feature selection. The BIN layer similarly contains an adaptive scale
%   and shift layer, and applies this along both the time-axis and dimension-axis and returns
%   a learned linear combination of these, while the DAIN layer only summary representation along
%   time-axis
We then looked at the most commonly used data preprocessing techniques, which are mostly static
distribution transformation such as min-max scaling, Z-score scaling, Z-score scaling with a
$\tanh$ activation function, decimal scaling, the Box-Cox transformation for handling skewed data,
and winsorization for handling outliers. All of these techniques use simple statistics such as the
mean, the standard deviation etc.\ to compute the transformation parameters. We then looked at
another more recent class of preprocessing techniques, adaptive methods, of which we considered the
\ac{DAIN}, \ac{RDAIN} and \ac{BIN} methods. These three adaptive preprocessing
methods have learnable parameters that are optimised together with the neural
network instead of using simple statistics. They are also all local-aware, meaning that they use
a summary representation of each sample to adjust the extent at which the normalization is applied
to each sample. The \ac{BIN} method does this over both the time- and dimension-axis while the
\ac{DAIN} method only uses a summary representation computed with a reduction over the time-axis.

% }}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Methods} %%%%       METHODS         %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Introduction
% Introduction {{{
In this chapter, I present three novel preprocessing methods. The first method, abbreviated
\acs{EDAIN}, is based on existing work by \citeauthor{dain} and \citeauthor{bin}. It
is an adaptive preprocessing method, so it performs a sequence of parametrised transformations
on the input data before passing it to a deep neural network. To optimize the adaptive layer,
the deep
neural network is augmented with the \acs{EDAIN} layer and both the neural network parameters
and the \acs{EDAIN} parameters are trained with stochastic gradient descent.
In \cref{sec:EDAIN-KL-method}, I present the second method, abbreviated \acs{EDAIN-KL}.
This method uses a very similar architecture to the \acs{EDAIN} layer, but instead of fitting the
parameters using stochastic gradient descent, it is optimised with a technique inspired by
\textit{normalizing flow} networks. In \cref{sec:PREPMIX-CAPS-method}, my third
contribution, the \acs{PREPMIX-CAPS} procedure, is presented. This procedure is significantly
different from the first two methods, as it automatically selects a mixture of static preprocessing
techniques to apply to the data instead of using adaptive transformations.

% }}}

\section{EDAIN}% EDAIN
% Methods: EDAIN {{{
\label{sec:EDAIN-method}

% Topics of this section:
% * Illustrative diagram of the 4 layers, and the weights, justifying order of operations
% * Differences to DAIN, explaining batch awareness, and specifics of (amex) dataset worked with
% * Explaining the 3 parts:
%   * outlier removal: Inspired by winsorization, plot of curve for different values,to show how work
%   * scale&shift: able to generalise standard scaling
%   * power transform: should also work for negative and positive values, while being numerically stable


My first contribution is the \ac{EDAIN} layer. This adaptive preprocessing layer is inspired
by the likes of \citep{dain}  and \citep{bin}, but unlike the aforementioned methods, the
\ac{EDAIN} layer also supports normalizing the data in a \textit{global-aware} fashion, whereas
the \ac{DAIN}, \ac{RDAIN} and \ac{BIN} layers are all \textit{local-aware}.
The \ac{EDAIN} layer has four different sublayers. The first sublayer reduces the effect of
outliers in the data, while the second and third sublayer perform and adaptive shift and
scale operation. Finally, an adaptive power transform operation is applied to reduce the
skewness of the input data.

\subsection{Architecture}%
\label{sub:Architecture}

% Latex equations used:
% \(\mathbf{\alpha}' \odot \left(\mathbf{\beta}' \odot \tanh\left\{(\mathbf{x}_t^{(i)}-\hat{\mathbf\mu}) \oslash \mathbf{\beta}'  \right\}+\hat{\mathbf\mu} \right)+\left(1-\mathbf{\alpha}' \right) \odot \mathbf{x}_t^{(i)}\)
% \((\mathbf{\tilde{x}}_t^{(i)}  - \mathbf{\gamma} \mathbf{\mu}_{\mathbf{\tilde{x}}_t^{(i)}}) \oslash \mathbf{\lambda} \sigma_{\mathbf{\tilde{x}}_t^{(i)}}, \textrm{ if local-aware} \)
% TODO: add w=(alpha, beta) above the red boxes to highlight which parameters are optimized...
\begin{figure}
\begin{center}
    \includegraphics[width=\textwidth]{diagrams/edain-diagram.pdf}
\end{center}
\caption{An overview of the architecture of the proposed \ac{EDAIN} normalization layer.}
\label{fig:edain-arch}
\end{figure}

An overview of the layer's architecture is shown in figure \cref{fig:edain-arch}.
Given some input time-series $\bfX^{(i)} \in \R^{d \times T}$, each temporal segment
$\bfx^{(i)}_t$ is passed through an adaptive outlier removal layer, followed by an adaptive shift
and scale operation, and then finally passed through an adaptive power transformation layer.
The architecture also has two modes, \textit{local-aware} and \textit{global-aware}. In
global-aware mode, the \ac{EDAIN} layer aims to normalize each input such that the resulting
distribution of all the samples in the dataset resemble a unimodal normal distribution, that is,
a ``global normalization''. In local-aware mode,
the \ac{EDAIN} layer's normalization operations
also depend on summary statistics of each input sample $\bfX^{(i)}$, and the goal is to transform
all the data into a common normalized representation space, independent of where in the
``global distribution'' the sample originated from. This mode is most suitable for
multi-modal input data, as samples from different modes can all be transformed into one common
normalized unimodal distribution. On the other hand, the global-aware mode
is most suitable if all the data comes from a similar data generation mechanism,
and works best if the input data has few modes.
In local-aware mode, the \ac{EDAIN} architecture is similar to the
\ac{DAIN} architecture proposed by \citeauthor{dain} and shown
in \cref{fig:dain-arch}, but it
extends it with both a global-aware mode as well as an adaptive outlier removal sublayer and
an adaptive power transform sublayer.

% Outlier removal
\subsubsection{Outlier removal}% {{{
\label{ssub:Outlier removal}

Handling outliers and extreme values in the dataset can increase predictive performance if done
correctly (citation needed). Two common ways of doing this are omission and winsorization
\citep{winsorization}. With the former, observations that are deemed to be extreme are simply
removed during training. With the latter, all the data is still used, but observations lying
outside a certain number of standard deviation from the mean, or below or above certain
percentiles, are \textit{clipped} to be closer to the mean or median of the data. We refer to this
technique as \textit{winsorization}.
For example, if winsorizing data using 3 standard deviation, all values less than
$\mu-3\sigma$ are set to be exactly $\mu-3\sigma$. Similarly, the values above
$\mu+3\sigma$ are clipped to this value. Winsorization can also be done using percentiles,
where common boundaries are the first and fifth percentiles \citep{winsorization}.
However, the type of winsorization, as well as the number of standard deviation
or percentiles to use, might depend on the dataset. Additionally, it might not
be necessary to winsorize the data at all if the outliers turn out to not
negatively affect performance. All this introduces more hyperparameters to tune
during modelling. The outlier removal operation presented here aims to automatically  determine both
whether winsorization is necessary for a particular feature, and determine the threshold at
which to apply winsorization.

For input vector $\bfx_t^{(i)} \in \R^d$, the adaptive outlier removal operation is defined as:
\begin{equation}\label{eq:adaptive-outlier-removal}
    \mathbf{h}_1(\bfx_t^{(i)})=\bm\alpha' \odot \underbrace{\left(\bm\beta' \odot
        \tanh\left\{\left(\bfx_t^{(i)}-\hat{\bm\mu}\right) \oslash \bm\beta'  \right\}+\hat{\bm\mu}
\right)}_{\text{smooth adaptive centred winsorization}}
    +\underbrace{\left(1-\bm\alpha' \right) \odot \bfx}_{\text{residual connection}},
\end{equation}
where
$\bm\alpha' \in [0,1]^d$ is a parameter controlling how much winsorization to apply to each feature,
and $\bm\beta' \in [\beta_{\text{min}},\infty)^d$ controls the winsorization threshold for
each feature, that is, the maximum absolute value of the output, thus controlling the range of the
output. The effect of the two parameters is illustrated in \cref{fig:adaptive_outlier}.
The unknown parameters of the model are $\bm\alpha \in \R^d$ and $\bm\beta \in \R^d$, and they
are transformed into the constrained parameters $\bm\alpha'$ and $\bm\beta'$, as used in
\cref{eq:adaptive-outlier-removal} through the following  mappings:
\begin{equation}
    \bm\alpha'=\frac{e^{\bm\alpha}}{1\oplus e^{\bm\alpha}} \qquad\qquad
    \bm\beta'=\beta_{\text{min}}\oplus e^{\bm\beta},
\end{equation}
where $\beta_{\textrm{min}} \in \R$ is a hyperparameter that can be tuned, but a suitable value is $\beta_{\textrm{min}}=1$. We introduce $\beta_{\textrm{min}}>0$ to prevent the sublayer from
squeezing all the data into the range $[0,0]$ during training, as the smallest possible range
with the parameter becomes $[-\beta_{\textrm{min}}, \beta_{\textrm{min}}]$.


The $\hat{\bm\mu}\in \R^d$ parameter in \cref{eq:adaptive-outlier-removal} is an estimate of the mean of the data, and is used
to ensure the winsorization is centred. When setting the \ac{EDAIN} layer in \textit{local-aware}
mode, it is simply the mean of the current batch of data points, $\mathcal{B}$:
\begin{equation}
    \hat{\mu}_k=\frac{1}{|\mathcal{B}| T} \sum_{i \in \mathcal{B}} \sum_{t=1}^T x_{t,k}^{(i)}, \qquad k=1,\dots,d.
\end{equation}
In \textit{global-aware} mode, it is iteratively updated using a \textit{cumulative
moving average estimate} at each forward pass of the sublayer.
This is to better approximate the global mean of the data.
% For ease of notation, we let $\mathbf{W}_1=(\bm\alpha, \bm\beta)$ denote the $2d$ unknown
% parameters that are optimised for the adaptive outlier removal layer.

\begin{figure}
\begin{center}
    \includegraphics[scale=1.0]{figures/adaptive_outlier_removal.pdf}
\end{center}
\caption{Plot of the adaptive outlier removal operation for different combinations of parameter
values for $\alpha'$ and $\beta'$.}
\label{fig:adaptive_outlier}
\end{figure}


% }}}

% Scale and shift
\subsubsection{Scale and shift}% {{{
\label{ssub:Scale and shift}

Depending on the dataset, one might want to aim for a \textit{global
normalization}, in which case a \textit{global-aware} scale and shift operation is most
suitable. If the dataset has many different modes, with significantly different distribution characteristics, a \textit{local normalization} based on the specific mode each data point comes from is more suitable, in which case a \textit{local-ware} scale and shift
operation works best. This gives two different approaches and scaling and shifting the data in an adaptive fashion.

\paragraph{Global-aware}%
\label{par:Global-aware}

In global-aware mode, the adaptive shift and scale layer, combined, simply performs the operation
\begin{equation}\label{eq:adaptive-scale-shift}
    \mathbf{h}_3(\mathbf{h}_2(\bfx_t^{(i)})):=(\bfx_t^{(i)} - \bm\gamma) \oslash \bm\lambda,
\end{equation}
where the unknown parameters are $\bm\gamma \in \R^d$ and $\bm\lambda \in (0,\infty)^d$.
This makes the scale-and-shift layer a generalised version of
Z-score scaling, or standard scaling, as setting
\begin{equation}
    \bm\gamma:=\frac{1}{N T}  \sum^{N}_{i=1} \sum^{T}_{t=1} \bfx_{t}^{(i)}
\end{equation}
and
\begin{equation}
    \bm\lambda:=\frac{1}{N T} \sum^{N}_{i=1} \sum^{T}_{t=1} \left(\bfx_{t}^{(i)}- \bm\gamma\right)^2
\end{equation}
makes the operation in \cref{eq:adaptive-scale-shift} equivalent to Z-score scaling.
This \textit{global-aware} mode is useful if the distribution is similar across batches
and constitute a global unimodal distribution that should be centred, as the operation can generalise Z-score scaling.

\paragraph{Local-aware}%
\label{par:Local-aware}

Some datasets might have multiple modes arising from significantly different
data generation mechanisms. Attempting to scale and shift each batch to a global mean and
standard deviation might hurt performance in such cases. Instead, \citeauthor{dain} propose
basing the scale and shift on a \textit{summary representation} of each data point, allowing
each sample to be normalized according the specific mode  of the data it originated from.
This gives
\begin{equation}
    \mathbf{h}_3\left(\mathbf{h}_2\left(\bfx_t^{(i)}\right)\right):=
    \left(\bfx_t^{(i)} - \left[\bm\gamma \odot \mu_{\bfx}^{(i)}\right]\right) \oslash \left[\bm\lambda \odot \sigma_{\bfx}^{(i)}\right],
\end{equation}
where the summary representations $\sigma_{\bfx}^{(i)} \in \R^d$ and $\mu_{\bfx}^{(i)} \in \R^d$ are computed through a reduction
along the temporal dimension of each observation:
\begin{align}
    \mu_\bfx^{(i)}&=\frac{1}{T} \sum^{T}_{t=1} \bfx^{(i)}_t  \\
    \sigma_\bfx^{(i)}&=\sqrt{\frac{1}{T}  \sum^{T}_{t=1} \left(\bfx^{(i)}_t- \mu_\bfx^{(i)} \right)^2}.
\end{align}
With this mode, it is difficult for the layer to generalise Z-score scaling, but it allows
discarding mode information such that highly multimodal distributions appear unimodal after passing
through the layer.

% }}}

% Power transform
\subsubsection{Power transform}% {{{
\label{ssub:Power transform}

Many real-world datasets exhibit significant skewness, which is often treated using power
transformations (citation needed). The most common transformation is the Box-Cox transformation,
but this is only valid for positive values, so it is not applicable to most real-world datasets
\citep{boxcox}. An alternative is a transformation proposed by \citeauthor{yeoJohnson},
known as the Yeo-Johnson transform:
\begin{equation}\label{eq:yeo-johnson}
    f_{\textrm{YJ}}^\lambda(x)= \left\{
        \begin{array}{ll}
            \frac{(x+1)^{\lambda}-1}{\lambda}, & \textrm{if } \lambda \neq 0, x \geq 0; \\
            \log(x + 1), & \textrm{if } \lambda = 0, x \geq 0;  \\
            \frac{(1-x)^{2-\lambda}-1}{\lambda-2} , & \textrm{if } \lambda \neq 2, x < 0; \\
            -\log(1-x), & \textrm{if } \lambda=2, x < 0.
        \end{array}
    \right.
\end{equation}
Like the Box-Cox transformation, the transformation $f_{\textrm{YJ}}$ only has one unknown parameter, $\lambda$, but
it works for any $x \in \R$, not just positive values \citep{yeoJohnson}.
The power transform layer simply applies the transformation in \cref{eq:yeo-johnson} along each dimension of the input, that is for each $i=1,\dots,N$ and $t=1,\dots,T$,
\begin{equation}
    \left[\mathbf{h}_4\left(\bfx_t^{(i)}\right)\right]_j:=f^{\lambda_j^{(\textrm{YJ})}}_{\textrm{YJ}}\left(x_{t,j}^{(i)}\right), \;\;\; j=1,\dots,d.
\end{equation}
The vector of the $d$ unknown parameter for the Yeo-Johnson transformation will be
denoted $\bm\lambda^{(\textrm{YJ})} \in \R^d$, as to not be confused with the scale parameter
$\bm\lambda \in \R^d$.


% }}}

% Optimising the parameters
\subsection{Optimisation through stochastic gradient descent}% {{{
\label{sub:edain_opt_sgd}

To optimise the unknown parameters
$\left( \bm{\alpha}, \bm{\beta}, \bm{\gamma}, \bm{\lambda}, \bm{\lambda}^{(\textrm{YJ})}\right)$,
the deep neural network is augmented by prepending the \ac{EDAIN} layer, as shown in
\cref{fig:edain-arch}. Then the input data is fed into the augmented model in batches,
as when training a neural network, and after each forward pass of the model, the weights
are updated through stochastic gradient descent while training the neural network.
As observed by \citeauthor{dain}, the model convergence is unstable if the same learning rate
$\eta \in \R$ that is used for training the deep neural network is also used for training all
the sublayers of the \ac{EDAIN} layer. Therefore, separate learning rate modifiers
$\eta_{\textrm{out}}$,
$\eta_{\textrm{shift}}$,
$\eta_{\textrm{scale}}$ and
$\eta_{\textrm{pow}}$ for the outlier removal, shift, scale and power transform sublayers are
introduced as additional hyperparameters and the weight updates happen according to the equation:
\begin{equation}
    \Delta \left( \bm{\alpha}, \bm{\beta}, \bm{\gamma}, \bm{\lambda}, \bm{\lambda}^{(\textrm{YJ})}\right)
    =
    -\eta\left(
        \eta_{\textrm{out}} \frac{\partial \mathcal{L}}{\partial \bm\alpha},
        \eta_{\textrm{out}} \frac{\partial \mathcal{L}}{\partial \bm\beta},
        \eta_{\textrm{shift}} \frac{\partial \mathcal{L}}{\partial \bm\gamma},
        \eta_{\textrm{scale}} \frac{\partial \mathcal{L}}{\partial \bm\lambda},
        \eta_{\textrm{pow}} \frac{\partial \mathcal{L}}{\partial \bm{\lambda}^{(\textrm{YJ})}}
    \right),
\end{equation}
where $\mathcal{L}$ denotes the criterion evaluated at a batch of inputs and targets.


% }}}

% }}}

\section{EDAIN-KL}% EDAIN-KL
% Methods: EDAIN-KL {{{
\label{sec:EDAIN-KL-method}

The \ac{EDAIN-KL} layer has a very similar architecture to the earlier-presented
\ac{EDAIN} layer, but the unknown parameter are optimised in a completely different
manner. Unlike the \ac{EDAIN} layer, the \ac{EDAIN-KL} layer is not attached to the deep neural
network during training and thus not trained simultaneously with the neural network. Instead,
before training the neural network, we train the \ac{EDAIN-KL} layer in isolation.
This is done by using it to transform a standard
normal distribution into a distribution that is similar to our training dataset. Then, after
the \ac{EDAIN-KL} weights have been optimized, we use the layer in reverse to normalize samples
from the training dataset before passing it to the neural network.

\subsection{Architecture}%
\label{sub:Architecture}

The \ac{EDAIN-KL} layer has a very similar architecture to the \ac{EDAIN} layer, described in
\cref{sec:EDAIN-method}, but the outlier removal transformation has been simplified to ensure its
inverse is analytic. Additionally, the layer no longer supports local-aware mode, as this
would make the inverse intractable. The \ac{EDAIN-KL} transformations are:
\begin{align}
    \textrm{(Outlier removal)} \qquad& \mathbf{h}_1\left(\bfx^{(i)}_t\right)=\bm\beta' \odot \tanh\left\{(\bfx^{(i)}_t - \hat{\bm\mu}) \oslash \bm\beta' \right\}+\hat{\bm\mu} \label{eq:kl1}\\
    \textrm{(shift)} \qquad& \mathbf{h}_2\left(\bfx^{(i)}_t\right)=\bfx^{(i)}_t- \bm\gamma \label{eq:kl2}\\
    \textrm{(scale)} \qquad& \mathbf{h}_3\left(\bfx^{(i)}_t\right)=\bfx^{(i)}_t \oslash \bm\lambda  \label{eq:kl3}\\
    \textrm{(power transform)} \qquad& \mathbf{h}_4\left(\bfx^{(i)}_t\right)=\left[
        f^{\lambda_1^ {(\textrm{YJ})} }_{\textrm{YJ}}\left(x^{(i)}_{t,1}\right)
        % \quad f^{\lambda_2^ {(\textrm{YJ})}}_{\textrm{YJ}}\left(x^{(i)}_{t,1}\right)
        \quad \cdots
        \quad f^{\lambda_d^ {(\textrm{YJ})}}_{\textrm{YJ}}\left(x^{(i)}_{t,d}\right)
    \right], \label{eq:kl4}
\end{align}
where $f^{\lambda_i^ {(\textrm{YJ})}}_{\textrm{YJ}}(\cdot)$ is defined in \cref{eq:yeo-johnson}.

\subsection{Optimisation through Kullback-Leibler divergence}%
\label{sub:Optimisation through Kullback-Leibler divergence}

The  optimisation approach used to train the \ac{EDAIN-KL} method is inspired by normalizing flow,
of which \citeauthor{normalizing_flows} provide a great overview of \cite{normalizing_flows}.
Before describing the approach, we provide a brief overview of related notation and some
background on the concept behind normalizing flows. After this, we go through how the
\ac{EDAIN-KL} layer itself can be treated as an invertible bijector to fit into the
normalizing flow framework. In doing so, we realize the need for analytic and differentiable
expressions for certain terms related to the \ac{EDAIN-KL} layer. Derivations for these terms are
then presented.

\subsubsection{Brief background on normalizing flow}%
\label{ssub:Brief background on normalizing flow}

The idea behind normalizing flow is taking a simple random variable, such as a standard
Gaussian, and transforming it into a more complicated distribution, for example, one that
resembles the distribution of a given dataset of samples.
Consider a random variable $\bfZ \in \R^d$ with a known and
analytic expression for the \ac{pdf} $p_\bfz: \R^d \rightarrow {\R}$. We refer to $\bfZ$ as
the \textit{base distribution}. We then define
a parametrised invertible function
$\mathbf{g}_{\bm \theta}:\R^d \rightarrow {\R^d}$, also known as a \textit{bijector},
and use this to transform the base distribution into a new
 probability distribution: $\bfY=\mathbf{g}_{\bm\theta}(\bfZ)$.
By increasing the complexity of the bijector $\mathbf{g}_{\bm\theta}$, by for instance using
a deep neural network, the transformed distribution $\bfY$ can grow arbitrarily complex as well.
The \ac{pdf} of the transformed distribution can then be computed using the change of variable
formula \citep{normalizing_flows}, where
\begin{align}
    p_\bfY(\bfy)&=p_\bfZ(\mathbf{g}_{\bm\theta}^{-1}\left(\bfy\right))\cdot \left|\det \mathbf{J}_{\bfY \rightarrow {\bfZ}}\left(\bfy \right) \right| \nonumber \\
                & = p_\bfZ(\mathbf{g}_{\bm\theta}^{-1}\left(\bfy\right))\cdot \left|\det \mathbf{J}_{\bfZ \rightarrow {\bfY}}\left( \mathbf{g}^{-1}_{\bm\theta}(\bfy) \right) \right|^{-1}, \label{eq:sj98days9dohao}
\end{align}
where $\mathbf{J}_{\bfZ \rightarrow {\bfY}}$ is the Jacobian matrix for the \textit{forward mapping}
$\mathbf{g}_{\bm\theta} : \bfz \mapsto \bfy$. Recall
that the $(i,j)$th entry of the Jacobian matrix of some multivariate function $\mathbf{f}$ is
given by $\mathbf{J}_{ij}=\frac{\partial f_i}{\partial x_j}$.
Taking logs on both sides of \cref{eq:sj98days9dohao}, it follows that
\begin{equation}\label{eq:logDetJac}
    \log p_\bfY(\bfy)= \log p_\bfZ(\mathbf{g}_{\bm\theta}^{-1}\left(\bfy\right)) - \log \left|\det \mathbf{J}_{\bfZ \rightarrow {\bfY}}\left(\mathbf{g}_{\bm\theta}^{-1}\left(\bfy\right) \right) \right|.
\end{equation}

One common application of normalizing flows is density estimation \citep{normalizing_flows};
Given a dataset $\mathcal{D}=\{\bfy^{(i)}\}_{i=1}^N$ with samples from some
unknown, complicated distribution, we want to estimate its \ac{pdf}.
This can be done with likelihood-based estimation, where we assume the data points
$\bfy^{(1)}, \bfy^{(2)},\dots,\bfy^{(N)}$ come from, say,
the parametrised distribution $\bfY=\mathbf{g}_{\bm \theta}(\bfZ)$ and
we optimise $\bm\theta$ to maximise the data log-likelihood,
\begin{align}
    \log p(\mathcal{D}| \bm\theta)
    & = \sum_{i=1}^N \log p_\bfY(\bfy^{(i)}| \bm\theta ) \\
    &\stackrel{\cref{eq:logDetJac}}{=} \sum^{N}_{i=1}
    \log p_\bfZ\left(\mathbf{g}_{\bm\theta}^{-1}\left(\bfy^{(i)}\right)\right) - \log \left|\det \mathbf{J}_{\bfZ \rightarrow {\bfY}}\left(\mathbf{g}_{\bm\theta}^{-1}\left(\bfy^{(i)}\right)\right) \right|. \label{eq:logProbKL}
\end{align}
This is equivalent to minimising the \ac{KL-divergence} between the empirical distribution
$\mathcal{D}$ and the transformed distribution $\bfY=\mathbf{g}_{\bm\theta}(\bfZ)$:
\begin{align}
    \argmax_{\bm\theta} \log p(\mathcal{D}| \bm\theta)
    &= \argmax_{\bm\theta}\sum_{i=1}^N \log p_\bfY \left(\bfy^{(i)}\big|\bm\theta \right) \\
    &=\frac{1}{N}  \sum_{i=1}^N \log p_{\mathcal{D}}\left(\bfy^{(i)}\right)
        +\argmax_{\bm\theta}\frac{1}{N} \sum_{i=1}^N \log p_\bfY \left(\bfy^{(i)}\big|\bm\theta \right) \\
    &= \argmin_{\bm\theta}\frac{1}{N}  \sum_{i=1}^N \log p_{\mathcal{D}}\left(\bfy^{(i)}\right)
    -\frac{1}{N} \sum_{i=1}^N \log p_\bfY \left(\bfy^{(i)}\big|\bm\theta \right) \\
    &= \argmin_{\bm\theta}\sum^{N}_{i=1}  p_\mathcal{D}\left(\bfy^{(i)}\right)  \log p_{\mathcal{D}}\left(\bfy^{(i)}\right) \\
    &\qquad- \sum_{i=1}^N  p_\mathcal{D}\left(\bfy^{(i)}\right) \log p_\bfY \left(\bfy^{(i)}\big|\bm\theta \right) \\
    &= \argmin_{\bm\theta} D_{\textrm{KL}}\left(\mathcal{D}\;||\; (\bfY\mid\bm\theta)\right).
\end{align}
When training an normalizing flow model, we want to find the parameter values
$\bm\theta$ that minimize the above \ac{KL-divergence}.
This is done using stochastic gradient descent and backpropagation, as described in
\cref{sec:Deep learning} where the criterion $\mathcal{L}$ is set to be the negation
of \cref{eq:logProbKL}. That is, the loss becomes the negative log likelihood of a batch
of samples from the training dataset. To perform optimisation with this criterion,
we need to compute all the terms in \cref{eq:logProbKL} and this expression needs to
be differentiable, as the backpropagation algorithm uses the gradient of the loss
with respect to the input data.
We therefore need to find
\begin{enumerate}[(i)]
    \item  an analytic and differentiable expression for the inverse transformation
$\mathbf{g}_{\bm\theta}^{-1}\left(\cdot \right)$,
    \item  an analytic and differentiable expression for the \ac{pdf} of the base distribution
$p_{\bfZ}(\cdot)$, and
    \item an analytic and differentiable expression for the log determinant of the Jacobian matrix for $\mathbf{g}_{\bm\theta}$, that is
$\log \left|\det \mathbf{J}_{\bfZ \rightarrow {\bfY}} \right|$.
\end{enumerate}
We will derive these three components for our \ac{EDAIN-KL} layer in the next section, but
before doing that, we make note of the following lemma.
Using a result stated in \citeauthor{normalizing_flows}, the following can be shown:
\begin{lemma}\label{thrm:normFlow}
    Let $\mathbf{g}_1,\dots, \mathbf{g}_n:\R^d \rightarrow {\R^d}$ all be bijective functions, and consider
    the composition of these functions, $\mathbf{g}=\mathbf{g}_n \circ \mathbf{g}_{n-1} \cdots \circ \mathbf{g}_1$.
    Then, $\mathbf{g}$ is a bijective function with inverse
    \begin{equation}
        \mathbf{g}^{-1}=\mathbf{g}_1^{-1} \circ \cdots \circ \mathbf{g}_{n-1}^{-1} \circ \mathbf{g}_n^{-1},
    \end{equation}
    and the log of the absolute value of the determinant of the Jacobian is given by
    \begin{equation}
        \log \left| \det \mathbf{J}_{\mathbf{g}^{-1}}(\cdot)\right|
        = \sum_{i=1}^N \log\left|\det \mathbf{J}_{\mathbf{g}_i^{-1}}(\cdot) \right|.
    \end{equation}
    Similarly,
    \begin{equation}
        \log \left| \det \mathbf{J}_{\mathbf{g}}(\cdot)\right|
        = \sum_{i=1}^N \log \left|\det \mathbf{J}_{\mathbf{g}_i}(\cdot) \right|.
    \end{equation}
\end{lemma}

\subsubsection{Application to EDAIN-KL}%
\label{ssub:Application to EDAIN-KL}

Like with the \ac{EDAIN} layer, we want to compose the outlier removal, shift, scale and power
transform transformations into one operation, which we do by defining
\begin{equation}\label{eq:gtheta}
    \mathbf{g}_{\bm\theta}=\mathbf{h}_1^{-1} \circ  \mathbf{h}_2^{-1} \circ \mathbf{h}_3^{-1} \circ \mathbf{h}_4^{-1},
\end{equation}
where $\bm\theta=(\bm\beta, \bm\gamma, \bm\lambda, \bm\lambda^{(\textrm{YJ})})$ and
$\mathbf{h}_1,\dots,\mathbf{h}_4$ are defined in \cref{eq:kl1,eq:kl2,eq:kl3,eq:kl4}, respectively.
Notice that we apply all the operations in reverse order, compared to the \ac{EDAIN} layer. This
is because we will use $\mathbf{g}_{\bm\theta}$ to transform our base distribution $\bfZ$ into
a distribution that resembles the training dataset, $\mathcal{D}$, not the other way around.
Then, to normalize the dataset after fitting the \ac{EDAIN-KL} layer, we apply
\begin{equation}\label{eq:gthetainv}
    \mathbf{g}_{\bm\theta}^{-1}=\mathbf{h}_4 \circ \mathbf{h}_3 \circ \mathbf{h}_2 \circ \mathbf{h}_1
\end{equation}
to each sample, similar to the \ac{EDAIN} layer.
It can be shown that all the transformations defined in
\cref{eq:kl1,eq:kl2,eq:kl3,eq:kl4} are invertible, of which a proof is given in
the next subsection.
Using \cref{thrm:normFlow}, it thus follows that
$\mathbf{g}_{\bm\theta}$, as defined in \cref{eq:gtheta}, is bijective and that its inverse
is given by \cref{eq:gthetainv}. Noticing that we already have analytic and differentiable
expressions for $\mathbf{h}_1$, $\mathbf{h}_2$, $\mathbf{h}_3$, $\mathbf{h}_4$ in
\cref{eq:kl1,eq:kl2,eq:kl3,eq:kl4}, the inverse of the bijector, $\mathbf{g}_{\bm\theta}^{-1}$,
defined in \cref{eq:gthetainv} also has an analytic and differentiable expression, so part
(i) is satisfied.

We now move onto deciding what our base distribution should be.
Making the input data as Gaussian as possible usually increases performance of deep sequence models
(citation needed), so a suitable base distribution is the standard multivariate Gaussian distribution
\begin{equation}
    \bfZ \sim \mathcal{N}(0, \mathbf{I}_d),
\end{equation}
whose \ac{pdf} $p_\bfZ(\cdot)$
has a nice analytic and differentiable expression, so part $(\textrm{ii})$ is satisfied.

In order to optimise the unknown parameters
$\bm\theta=(\bm\beta, \bm\gamma, \bm\lambda, \bm\lambda^{(\textrm{YJ})})$ of the
\ac{EDAIN-KL} layer by treating it as a normalizing flow bijector, we need an analytic and
differentiable expression for the right-hand side of \cref{eq:logProbKL}. We already have
an expression for part (i) and part (ii), so only part  (iii) remains.
That is, an analytic and differentiable expression for
the log of the determinant of the Jacobian matrix of $\mathbf{g}_{\bm\theta}$, $\log\left|\det \mathbf{J}_{\bfZ \rightarrow {\bfY}} \right|$. We will derive this
in the next subsection. Once that is done, parts (i), (ii) and (iii) are satisfied, so
$\bm\theta$ can be optimised using back-propagation as described in \cref{sec:Deep learning},
using the negation of \cref{eq:logProbKL} as the objective. In other words, we can optimise
$\bm\theta$ to maximise the likelihood of the training data under the assumption that it comes from
the distribution $\bfY= \mathbf{g}_{\bm\theta}(\bfZ)$. This is desirable, as if we can achieve
a high data likelihood, the samples $\bfy$ will more closely resemble a standard normal distribution
after being transformed by $\mathbf{g}_{\bm\theta}^{-1}$ after fitting the bijector.
This might then increase the performance as the neural network will be fed data
that is more Gaussian.

\subsubsection{Derivation
of %$\mathbf{g}_{\bm\theta}^{-1}$ and
$\log \left|\det \mathbf{J}_{\bfZ \rightarrow {\bfX}}\right|$}%
\label{ssub:ildj}

Recall that the \ac{EDAIN-KL} architecture is just a bijector that is composed of 4 other bijective
functions. Using the result in \cref{thrm:normFlow}, we get
\begin{equation}
    \log \left|\det \mathbf{J}_{\bfZ \rightarrow {\bfY}}(\cdot)  \right|
    = \sum^{4}_{i=1} \log \left|\det \mathbf{J}_{\mathbf{h}_i^{-1}}(\cdot) \right|.
\end{equation}
Considering the transformations in \cref{eq:kl1,eq:kl2,eq:kl3,eq:kl4}, we notice that all the
transformation happen element-wise, so for $i\in\{1,2,3,4\}$, we have
$\left[\frac{\partial \mathbf{h}_i^{-1}(\bfx)}{\partial x_{k}}\right]_j =0$ for $k \neq j$.
Therefore, the Jacobians are diagonal matrices, so the determinant is just the product of the
diagonal entries, giving
\begin{align}
    \log \left|\det \mathbf{J}_{\bfZ \rightarrow {\bfY}}(\bfx)  \right|
    & = \sum^{4}_{i=1} \log \left| \prod_{j=1}^d \left[\frac{\partial \mathbf{h}_i^{-1}(\bfx)}{\partial x_j}\right]_j   \right| \\
    & = \sum^{4}_{i=1} \sum_{j=1}^d \log \left| \left|\frac{\partial \mathbf{h}_i^{-1}(\bfx)}{\partial x_j}\right|_j   \right| \\
    & = \sum^{4}_{i=1} \sum_{j=1}^d \log \left| \frac{\partial h_i^{-1}\left(x_j;\theta_j^{(i)}\right)}{\partial x_j}  \right|, \label{eq:sgas9fg8a9sd8}
\end{align}
where in the last step we used the fact that $h_1,h_2,h_3$ and $h_4$ are applied element-wise
to introduce the notation $h_i(x_j;\theta^{(i)}_j)$ that means applying $\mathbf{h}_i$ to
some vector where the $j$th element is $x_j$, and the corresponding $j$th transformation
parameter takes the value $\theta^{(i)}_j$. For example, for the scale
function, $\mathbf{h}_3(\bfx)=\bfx \oslash \bm\lambda$, we have
$h_3(x_j;\lambda_j)=\frac{x_j}{\lambda_j}$.
From \cref{eq:sgas9fg8a9sd8}, we know that we only need to derive the derivatives for
the element-wise inverses, which we will now do for each of the four transformations.
In doing so, we also demonstrate that each transformation is bijective.

\paragraph{Shift}%
\label{par:Shift}

We first consider $h_2(x_j;\gamma_j)=x_j-\gamma_j$. Its inverse is $h_2^{-1}(z_j;\gamma_j)=z_j+\gamma_j$, and it follows that
\begin{equation}
    \log \left|\frac{\partial h_2^{-1}(z_j ; \gamma_j)}{\partial z_j} \right|
    = \log 1 = 0.
\end{equation}

\paragraph{Scale}%
\label{par:Scale}

We now consider $h_3(x_j;\lambda_j)=\frac{x_j}{\lambda_j}$, whose inverse is $h_3^{-1}(x_j;\lambda_j)={z_j}{\lambda_j}$. It follows that
\begin{equation}
    \log \left|\frac{\partial h_3^{-1}(z_j ; \gamma_j)}{\partial z_j} \right|
    = \log \left|{\lambda_j}  \right|.
\end{equation}

\paragraph{Outlier removal}%
\label{par:Outlier removal}

We now consider $h_1(x_j;\beta'_j)= \beta'_j \tanh\left\{\frac{(x_j - \hat{\mu}_j)}{\beta'_j}  \right\} + \hat{\mu}_j$. Its inverse is
\begin{equation}
    h_1^{-1}(z_j;\beta_j') =\beta' \tanh^{-1} \left\{\frac{z_j - \hat{\mu}_j}{\beta'_j}  \right\}
    +\hat{\mu}_j.
\end{equation}
It follows that
\begin{equation}
    \log \left|\frac{\partial h_1^{-1}(z_j ; \beta_j')}{\partial z_j} \right|
    = \log \left| \frac{1}{1-\left( \frac{z_j-\hat{\mu}_j}{\beta_j'}  \right)^2}  \right|
    = -\log\left| 1-\left( \frac{z_j-\hat{\mu}_j}{\beta_j'}  \right)^2 \right|.
\end{equation}

\paragraph{Power transform}%
\label{par:Power transform}

By considering the expression in \cref{eq:kl4}, it can be shown that for fixed
$\lambda^{(\textrm{YJ})}$, negative inputs are always
mapped to negative values and vice versa, which makes the Yeo-Johnson transformation invertible.
Additionally, in $\mathbf{h}_4(\cdot)$ the Yeo-Johnson transformation is applied element-wise, so
we get
\begin{equation}
    \mathbf{h}_4^{-1}(\mathbf{z})=\left[
        \left[f_{\textrm{YJ}}^{\lambda_1^{(\textrm{YJ})}}\right]^{-1}\bigg(z_1\bigg) \quad
        \left[f_{\textrm{YJ}}^{\lambda_2^{(\textrm{YJ})}}\right]^{-1}\bigg(z_2\bigg) \quad \cdots \quad
    \left[f_{\textrm{YJ}}^{\lambda_d^{(\textrm{YJ})}}\right]^{-1}\bigg(z_d\bigg) \right],
\end{equation}
where it can be shown that the inverse Yeo-Johnson transformation for a single element is given by
\begin{equation}
    \left[f_{\textrm{YJ}}^\lambda\right]^{-1}\bigg(z\bigg)= \left\{
    \begin{array}{ll}
        (z \lambda + 1)^{1/\lambda} -1, & \textrm{if } \lambda \neq 0, z \geq 0; \\
        e^z-1, & \textrm{if } \lambda = 0, z \geq 0;  \\
        1-\left\{1-z(2-\lambda)\right\}^{1/ (2-\lambda)} , & \textrm{if } \lambda \neq 2, z < 0; \\
        1-e^{-z}, & \textrm{if } \lambda=2, z < 0.
    \end{array}
    \right.
\end{equation}

The derivative with respect to $z$ then becomes
\begin{equation}
    \frac{\partial \left[f_{\textrm{YJ}}^\lambda\right]^{-1}(z)}{\partial z}= \left\{
        \begin{array}{ll}
            (z \lambda + 1)^{(1-\lambda)/\lambda}, & \textrm{if } \lambda \neq 0, z \geq 0; \\
            e^z, & \textrm{if } \lambda = 0, z \geq 0;  \\
            \left\{1-z(2-\lambda)\right\}^{(\lambda-1)/(2-\lambda)} , & \textrm{if } \lambda \neq 2, z < 0; \\
            e^{-z}, & \textrm{if } \lambda=2, z < 0.
        \end{array}
    \right.
\end{equation}
It follows that
\begin{equation}
    \log \left|\frac{\partial \left[f_{\textrm{YJ}}^\lambda\right]^{-1}(z)}{\partial z} \right|= \left\{
        \begin{array}{ll}
            \frac{1-\lambda}{\lambda}\log (z \lambda + 1), & \textrm{if } \lambda \neq 0, z \geq 0; \\
            z, & \textrm{if } \lambda = 0, z \geq 0;  \\
            \frac{\lambda - 1}{2-\lambda}\log\left\{1-z(2-\lambda)\right\} , & \textrm{if } \lambda \neq 2, z < 0; \\
            -z, & \textrm{if } \lambda=2, z < 0,
        \end{array}
    \right.
\end{equation}
which we use as the expression for $\log \left| \frac{\partial h_4^{-1}\left(z_j;\lambda_j^{(\textrm{YJ})}\right)}{\partial z_j} \right|$ for $z=z_1,\dots,z_d$.

Putting all of these expression together, we have an analytical and differentiable expression
for $\log\left| \det \mathbf{J}_{\bfZ \rightarrow {\bfY}}(\bfx) \right|$.

% TODO: write out the expression here.... ?

% }}}

\section{PREPMIX-CAPS}% PREPMIX-CAPS
% Methods: PREPMIX-CAPS {{{
\label{sec:PREPMIX-CAPS-method}

% TODO: find somewhere to mention this...
% Due to the computational complexity of the number of variables, the preprocessing of the
% multivariate time-series will be done \textit{across time}, that is, the same preprocessing method
% is applied to each of the variables, regardless of their timestep  index.


Unlike the \ac{EDAIN} and \ac{EDAIN-KL} layers, The \ac{PREPMIX-CAPS} procedure
is not an adaptive preprocessing technique. Instead, it can be thought of as an automated way
of selecting the best static preprocessing technique for each predictor variable in the dataset.
Say we are working with multivariate time-series dataset where each time-series is of length $T$
and dimensionality $d$, that is, we have $d$ predictor variables.
The \ac{PREPMIX-CAPS} procedure starts with \textit{clustering phase},
producing $k$ clusters of the $d$ predictor variables,
where $k$ is a hyperparameter. There are two methods of clustering, one based on statistics
computed for each variable, and one based on the variables' relative \ac{KL-divergence}.
After the clustering has been performed, a static preprocessing method needs to be selected
for each cluster. This is done in an \textit{experiment running phase}. This phase has also
been  optimised with parallel computation, hence the ``parallel search'' in the procedure's
name.

\subsection{Clustering the predictor variables}%
\label{sub:prepmix-clustering}

We are working with a dataset of multivariate time-series,
$\mathcal{D}=\{\bfX^{(i)} \in \R^{d \times T}\}_{i=1,2,\dots,N}$.  In the
clustering phase, we want to determine how to segment the set of variables
$\{1,2,\dots,d\}$ into $k$ clusters such that the distribution of the
variables, according to $\mathcal{D}$, is as ``similar'' as possible within
each cluster.  The motivation behind this is that applying the same
preprocessing technique to similarly-distributed variables will have similar
effects on the neural network's performance, when trained on these preprocessed
variables.  To achieve a clustering where the distribution characteristics
within each clusters is as ``similar'' as possible, I propose two approaches:
Clustering based on distribution statistics, and an information theoretic clustering approach.

\subsubsection{Clustering based on statistics}%
\label{sub:Clustering based on statistics}

The first clustering approach is based on statistics. With this approach, we first compute
$d_{\textrm{stats}}$ different statistics for each of the $d$ predictor variables in the dataset $\mathcal{D}$.
This then gives a vector of $d_{\textrm{stats}}$ features for each of the $d$ predictor variables,
that can later be used as features in a clustering routine such as $K$-means. In this clustering
method, we have $d_{stats}=6$, and these statistics have been designed to quantitatively
capture a wide set of characteristics a distribution might have.

I will now present the six statistics that are computed
for each of the $d$ predictor variables.
The first statistic used is the Fisher's moment
coefficient of skewness \citep{shape}, which for $k=1,2,\dots,d$ is computed as
\begin{equation}\label{eq:fglkhy809s}
    \gamma_k=\frac{m_3}{m_2^{3/2}}, \quad \textrm{where }
    m_i=  \frac{1}{NT} \sum^{N}_{i=1} \sum^{T}_{t=1} \left(x^{(i)}_{t,k}-\mu_k \right)^i,
\end{equation}
where $\bm{\mu}=\frac{1}{NT}\sum^{N}_{i=1} \sum^{T}_{t=1} \bfx^{(i)}_t \in \R^d$ is the mean along the dimension-axis.
The second statistic used is the kurtosis \citep{shape}, which for $k=1,2,\dots,d$ is computed as
\begin{equation}
    \alpha_k=\frac{m_4}{m_2^2},
\end{equation}
where $m_i$ for $i\in \{2,4\}$ is defined in  \cref{eq:fglkhy809s}.
The third statistic used is the standard deviation, computed as
\begin{equation}
    \sigma_k=\sqrt{m_2},
\end{equation}
where $m_2$ is defined in \cref{eq:fglkhy809s}.

% Binning statistics
The next three statistics are designed to capture characteristic of the variables' \ac{pdf}, but
since this is unknown, we approximate it by \textit{binning} the samples in the dataset
$\mathcal{D}$ in $n_{\textrm{num. bins}}$ distinct bins for each variable $k=1,2,\dots,d$.
We note that is done after applying Min-Max scaling on the dataset so that all the samples take
values in the range $[0,1]$.
Then consider $\mathbf{B} \in \R^{d \times \textrm{num. bins}}$,
where $B_{k,m}$ denotes the number of samples from the set of values corresponding to the
$k$th predictor, that is $\left\{x_{t,k}^{(i)} \right\}_{i=1,\dots,N,\;t=1,\dots,T}$,
that fall into the $m$th bin.
After performing this binning operation to get $\mathbf{B}$, the fourth statistic is computed as
\begin{equation}
    \frac{1}{n_{\textrm{num. bins} }}  \argmax_{i} B_{k,i},
\end{equation}
which approximates the normalized location of the highest mode in the variable's \ac{pdf}.
The fifth static is computed as
\begin{equation}
    \frac{1}{n_{\textrm{num. bins} }}  \sum^{n_{\textrm{num. bins} }}_{i=1} \mathbb{I}\left\{
        B_{k,i} > 0
    \right\},
\end{equation}
approximating how many unique values the distribution has. The sixth statistic is computed as
\begin{equation}
     \max_{i} B_{k,i},
\end{equation}
denoting the density in the highest mode in the \ac{pdf}.
After computing all the statistics and compiling the matrix
$\bfX' \in \R^{d \times d_{\textrm{stats} }}$ where the rows are feature vectors corresponding
to each predictor variable, we apply $K$-means clustering to get $k$ clusters
of the $d$ predictor \citep{kmeans}. However, before doing this, we perform Z-score scaling
on $\bfX'$ to ensure all the $d_{\textrm{stats}}$ are equally weighted in the clustering
routine.

\subsubsection{Clustering based on KL-divergence}%
\label{sub:Clustering based on KL-divergence}

The second clustering method is based on information theory.
One approach to putting ``similar'' variables in the same cluster is to cluster based on the
relative \ac{KL-divergence} between each variable.
To do this, we start by constructing a
distance matrix $\mathbf{W} \in \R^{d \times d}$ where $W_{i,j}$ denotes the
\ac{KL-divergence} between variable $X_j$ and $X_i$ for $j > i$. From
\citep{mackay} we can compute the \ac{KL-divergence} between predictor variable $X_j$ and $X_i$
with
\begin{equation}
    W_{i,j}= \sum^{n_{\textrm{num. bins} }}_{k=1} \mathbb{P}_{X_i}\left( \frac{k}{n_{\textrm{num. bins} }}  \right)
    \log\left\{
    \mathbb{P}_{X_i}\left( \frac{k}{n_{\textrm{num. bins} }}  \right) \bigg/
    \mathbb{P}_{X_j}\left( \frac{k}{n_{\textrm{num. bins} }}  \right)
\right\},
\end{equation}
where $\mathbb{P}_{X_i}(\cdot)$ is an approximation of the \ac{pdf} of the $i$th predictor variable.
We get this approximation by binning the samples after performing Min-Max normalization to ensure
the samples all fall in the range $[0,1]$, as was done when clustering based on statistics.
The integer $n_{\textrm{num. bins}}$ denote the number of bins used when doing this.
This distance matrix $\mathbf{W}$ is then used together with an \textit{agglomerative clustering}
approach to cluster the $d$ variables into $k$ clusters
\citep{hierarchical_clustering}. Since the distance matrix $\mathbf{W}$ is
non-Euclidean, the linkage criteria used was selected to be ``average''.

% TODO: does the reader really know what linkage criteria and anglomerative clustering is????

\subsection{Determining the optimal preprocessing method for each cluster}%
\label{sub:Determining the optimal preprocessing method for each clusterch cluster}

\begin{figure}[htp]
    \vspace*{-15pt}
    \begin{center}
        \includegraphics[width=\textwidth]{diagrams/prepmix-diagram.pdf}
    \end{center}
    \caption{Illustration of ``ablation studies'' done for finding the optimal preprocessing method
    for each cluster, as part of the \ac{PREPMIX-CAPS} routine.}
    \label{fig:prepmix}
    \vspace*{-15pt}
\end{figure}

The goal of the \ac{PREPMIX-CAPS} preprocessing approach is preprocessing the
data using the mixture of preprocessing technique that gives the best
performance according to some validation metric. Usually, this is the
validation loss of the neural network being trained. As such, to select which
of the $f_0, f_1,\dots,f_{m-1}$ preprocessing techniques to apply to each
cluster, we need train the model from scratch for each combination of
preprocessing technique applied, in order to get the final validation loss.
We refer to this as one \textit{experiment}.
Recall that after clustering, we have $k$ clusters of variables and $m$ different
preprocessing methods to consider for each cluster. Trying all of the possible
combinations would require performing $m^k$ experiments, which is
computationally infeasible for large $k$ or $m$, especially if model training
is slow. Instead, we iteratively look at the isolated effect each of the
different preprocessing techniques have on a particular cluster, and repeat
this $k$ times, similar to an ablation study. This process is illustrated in
\cref{fig:prepmix}. For the clusters not being
considered in a particular experiment, a baseline preprocessing technique such
as standard scaling is applied to that cluster, as this technique in general
works well for most datasets (citation needed). This scheme reduces the number
of experiments from $m^k$ to $(m-1)k+1$, as we also do one experiment where the
baseline preprocessing technique is applied to all clusters. The scheme is
illustrated in \cref{fig:prepmix}, where we picked standard scaling as the
baseline preprocessing technique.

After these $(m-1)k+1$ experiments have been run, and the final validation loss has been recorded
for each experiment, we can analyse the results to determine what mixture of preprocessing
techniques to use. With \cref{fig:prepmix} as reference, let $\mathcal{L}_{C_{i},f_j}$
denote the validation loss associated with the experiment where
preprocessing method $f_j$, with $j>0$, was applied to cluster $C_i$.
For $C_1,\dots,C_k$, the validation loss
$\mathcal{L}_{C_i,f_0}$ is the validation loss from experiment 0, that is, the baseline experiment.
Then, the preprocessing method for cluster $C_i$ in the final mixture
is set to be $f_{\widehat{j}}$, where
\begin{equation}
    \widehat{j}=\argmin_{0 \leq j < m}  \mathcal{L}_{C_i,f_j}.
\end{equation}
This way of selecting the overall mixture based on separate marginal improvements in performance
makes the assumption
that the marginal improvements are not dependent on how variables in a different cluster are
preprocessed.
% TODO: comment on validity of this assumption?

\subsubsection{Optimisations}%
\label{ssub:Optimisations}

The different experiments, as shown in \cref{fig:prepmix}, have no dependencies
between them and can thus be executed in parallel. This allows speeding up the
experiment running phase through parallel computation.
Before starting the experiments, the set of \acp{GPU}
to use has to be configured, which we denote as $\mathcal{I}_{\textrm{device IDs}}$.
The number of jobs
to run concurrently on each \ac{GPU} at any point in time, denoted $n_{\textrm{num. jobs}}$, must
also be specified.
To allow parallel computation, all the experiments---or jobs---were encapsulated in a Python
\texttt{threading.Thread} object. The jobs were then allocated to the \acp{GPU} in
$\mathcal{I}_{\textrm{device IDs}}$ in a \textit{round-robin} fashion, that is, allocate the first
job to the first \ac{GPU}, the second job to the second \ac{GPU}, etc., wrapping around to the first
\ac{GPU} once we reach the last \ac{GPU}. This is done until up to
$\# \mathcal{I}_{\textrm{device IDs}} \cdot n_{\textrm{num. jobs}}$ have been allocated and
set to start executing.
When these jobs finish, subsequent experiments are scheduled in a similar fashion
fashion. Unlike standard \textit{round-robin} scheduling, each job is run until completion instead
of switching while they execute.

% \subsection{Hyperparameters}%
% \label{sub:Hyperparameters}
%
% For my experiments, I used the following selection of preprocessing techniques:
% \begin{itemize}
%     \item Standard scaling \textit{with time- and dimension-axis}
%     \item Standard scaling \textit{across time}
%     \item Standard scaling followed by $\tanh(\cdot)$ \textit{with time- and dimension-axis}
%     \item Standard scaling followed by $\tanh(\cdot)$ \textit{across time}
%     \item Min-Max scaling to $[0,1]$ \textit{with time- and dimension-axis}
%     \item Min-Max scaling to $[0,1]$ \textit{across time}
% \end{itemize}
% This gives $m=6$. From hyperparameter tuning on $k$, also found $k=20$ for amex dataset
% (TODO: don't go into datasets yet...).
% For both clustering methods, the number of bins parameter, required for computing some of the
% statistics, as well as estimating the \ac{pdf} values required for estimating the
% \ac{KL-divergence} between the random variables, was set to be
% $n_{\textrm{num. bins}}=5000$. The number of clusters $k$ to use, was tuned using the specific
% dataset the \ac{PREPMIX-CAPS} method was applied to, which is described in section TODO.

% }}}

\section{Conclusion}% Conclusion
\label{sec:Conclusion}% {{{

We have now looked at three different novel preprocessing methods,
\ac{EDAIN}, \ac{EDAIN-KL}, and \ac{PREPMIX-CAPS}. The \ac{EDAIN} layer starts by applying
an adaptive outlier removal transformation, followed by an adaptive shift and scale operation,
and finally and adaptive power transform operation to reduce skewness. The layer also has
two modes, \textit{local-aware} and \textit{global-aware}, designed to handle highly
multimodal data and data with fewer modes, respectively. To optimise the parameters of
the \ac{EDAIN} layer, it is prepended to an existing neural network and the
\ac{EDAIN} parameters and neural network parameters are the simultaneously optimised using
stochastic gradient descent. We then looked at the \ac{EDAIN-KL} layer, which has the same
four sublayers as \ac{EDAIN}, but instead of optimising these using gradient descent, the layer
is treated as a bijector. Then it is optimized by minimizing the \ac{KL-divergence} between the
training data and a standard  normal distribution, transformed by the \ac{EDAIN-KL} layer.
After this, the training data is normalized by applying the inverse transformation.
The final method we looked at was the \ac{PREPMIX-CAPS} procedure, which is an automated
pipeline for selecting which static preprocessing technique to apply to each variable. It does this
by first clustering all the predictor variables. Then, through a parallel experiment
running phase, it selects the preprocessing method that minimizes the validation loss for each
cluster.

% }}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Results} %%%%         Results        %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TODO: introduction to this chapter




\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation methodology}%%%  Evaluation methodology %%
\label{sec:Evaluation methodology}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Small introduction

% Sequence model architecture
\subsection{Sequence model architecture}% {{{
\label{sub:Sequence model architecture}

% }}}

% Fitting the models
\subsection{Fitting the models}% {{{
\label{sub:Fitting the models}

Mention scheduling, early stopping, optimizer used, learning rate etc.
% }}}

% Tuning adaptive preprocessing model hyperparameters
\subsection{Tuning adaptive preprocessing model hyperparameters}% {{{
\label{sub:Tuning adaptive preprocessing model hyperparameters}

Details on the tuning for all the methods presented
% }}}

% Evaluation metric
\subsection{Evaluation metrics}% {{{
\label{sub:Evaluation metrics}

% }}}

% Cross-validaiton
\subsection{Cross-validation}% {{{
\label{sub:Cross-validation}

Mention both how cross-validation is done on American Express dataset, and how done differently
on Limit Order Book dataset.

% }}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simulation study}%%%  Simulation study   %%
\label{sec:Simulation study}%%%%%%%%%%%%%%%%%%%%%%%%%

Small introduction, including motivation

% Multivariate time-series data generation algorithm
\subsection{Multivariate time-series data generation algorithm}% {{{
\label{sub:Multivariate time-series data generation algorithm}



% }}}

% Negative effects of irregularly-distributed data
\subsection{Negative effects of irregularly-distributed data}% {{{
\label{sub:Negative effects of irregularly-distributed data}


% }}}

% Preprocessing method experiments
\subsection{Preprocessing method experiments}% {{{
\label{sub:Preprocessing method experiments}



% }}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{American Express default prediction dataset}%%%  Amex dataset %%%
\label{sec:American Express default prediction dataset}%%%%%%%%%%%%%%%%%%%%

TODO

% Description
\subsection{Description}% {{{
\label{sub:Description}

The dataset from \cite{amex-data}, which we will abbreviate as the Amex
dataset, contains $N=458913$ customers (TODO: check?), entries of length $T=13$
$d=188$ features.  For each customer, there is a binary label $y \in \{0,1\}$
indicating whether the customer defaulted or not, that is, whether they were
able to pay back their credit card balance amount within 120 days after their
latest statement date \citep{amex-data}.  In the dataset provided by
\cite{amex-data}, the non-default records have been \textit{down-sampled} such
that final default rate in the dataset is about 25\%. The problem task is to
predict the probability that a customer will be default or not, that is
$\mathbb{P}(Y=1)$.  To do this, we are given $d=188$ aggregated profile
features, recorded at up to $T=13$ different credit card statement dates.  The
feature names have all been anonymized, but they can be categorised into the
five categories: delinquency variables, spend variables, payment variables,
balance variables and risk variables \citep{amex-data}.  Additionally, 11 of
the 188 features are categorical, so only the 177 numerical features will be
considered during preprocessing experiments in this thesis.


% talk about motivation, why stuiable for preprocessing research, refer to EDA
% plots etc.
The Amex dataset is very suitable for research on preprocessing techniques for
deep learning.  The dataset exhibits many traits commonly observed in
real-world datasets, such as skewed distributions, multiple modes, unusual
peaks, and extreme values (citation needed). It also has a lot of missing values, also common in
real-world dataset (citation needed). As such, if one discovers novel preprocessing methods that
work well on this dataset, they are likely to generalise well to other real-world datasets as well.
% TODO: include some EDA plots for this

% }}}

% Initial data preprocessing
\subsection{Initial data preprocessing}% {{{
\label{sub:Data formatting}

For privacy reasons, before data published by \cite{amex-data}, it was Min-Max
normalized to $[0, 1]$ and then an additive noise $\bm\epsilon \sim
\textrm{U}[-0.01,+0.01]$ was added to each variable. For more accurate
experimentation, I tried to undo this process: First, cite Raddar, denoise
numerical and categorical features, then randomly generate scale and shift of
data to undo Min-Max normalization, done using parameters TODO TODO

Raw dataset has a lot of missing values, some variables up to 90\% missing
values.  Across the whole dataset, 8.50\% of the numeric data points are
missing. Five variables also have more than 91\% of their data points missing,
and only 138 out of the 177 numeric variables have less than 1\% missing
values.  Despite missing values being a problematic issue in deep learning
(citation needed), this project will not focus on this issue.  Instead, we
focus on preprocessing techniques applied to a dataset where the missing values
have already been handled. However, our neural network cannot process input
data that is not fully numeric, so we will need to handle the missing values,
since every single time-series has at least one missing value. One common way
of doing this is filling (citation needed), where missing values are simply
replaced with a constant. Since the data is normalized to fall in the range
$[0,1]$, I chose to fill all missing values with the value $-0.5$, to create
some distance between existing values and missing ones. For the categorical
entries, the missing values are padded with -1.

In addition to missing values, some customers also have fewer than 13 credit
statements recorded. This makes some time-series shorter than others in the
dataset, which is problematic for the preprocessing techniques that are applied
with the time- and dimension-axis.  Additionally, it makes batching the data
cumbersome. Therefore, the shorter time-series are \textit{padded} with numeric
constants such that they are all of length $T=13$. For this, I chose to pad the
numeric values with -1 and the categorical values with -2, to separate padded
values and missing values.


The final dataset $\mathcal{D}=\{ \bfX^{(i)} \in \R^{d \times T}\}$ with $N=?$

% }}}

% Evaluation methodology
\subsection{Evaluation methodology}% {{{
\label{sec:amex_meth}

Before looking at the preprocessing experiment results on the Amex dataset in \cref{sub:amex_res},
we first describe how the experiments are conducted, involving the choice of sequence model,
and how it is trained and evaluated. In each experiment, we split the training data into five
20\% splits. We then train and evaluate the model using 5-fold cross-validation, which works
as follows: If relevant, we fit the preprocessing method on the 80\% training data split and use
it to transform all of the data. The sequence model is the trained using the 80\% training data
split, and its prediction on the remaining 20\% are used for evaluation using our metrics.
This is repeated for each of the five splits, giving us five different validation losses and
evaluation metric values.
This subsection is structured as follows.
We first present the architecture of the deep sequence model used for predicting the probability of
a default event for each customer. We then move onto describing the hyperparameters associated with
optimising this model, including choices such as loss function and choice of optimizer. Then we
go over the evaluation metrics used for this particular dataset.


\subsubsection{Sequence model}%
\label{ssub:Sequence model}

We chose to use a relatively simple \ac{RNN} sequence model, with a classifier head, as our
baseline model.
It consists of two stacked \ac{GRU} \ac{RNN} cells, both with a hidden dimensionality of 128.
We also use a dropout with 20\% probability. We also have 11 categorical features. These are
passed through seperate embedding layers, each with a dimensionality of 4. The outputs of the
embedding and numeric columns passed through the two \ac{GRU} units are then combined and
passed through the classifier head, which consists of 2 linear units with 128 and 64 units,
respectively, and seperated by a \ac{ReLU} activation function. The output is then fed through
a linear layer with a single output neuron, followed by a sigmoid activaiton function to constrain
the output to be a probability in the range $(0,1)$. Then we can feed in a time-series
$\bfX^{(i)} \in \R^{d \times T}$ to the model and get a probability $p_i \in (0,1)$ as the
output.

\subsubsection{Optimising the model}%
\label{ssub:Optimising the model}

Since the targets are binary labels $y_1,y_2,\dots,y_N \in \{0,1\}$ and our predictions are
probabilities $p_1,p_2,\dots,p_N \in (0,1)$, a suitable loss is binary cross entropy loss.
This gives the criterion
\begin{equation}
    \mathcal{L}(p_i,y_i)=- \left\{y_i \log p_i + (1-y_i) \log\left( 1- p_i \right)\right\}.
\end{equation}
After preprocessing the data according to whatever method being experimented on, the data is fed
in batches to the \ac{GRU} \ac{RNN} model and its unknown parameters are optimised according
to the description in \cref{sec:Deep learning}. This was done using a batch size of 1024 and
a base learning rate of $\eta=10^{-3}$. The optimizer used was the Adam optimizer proposed by
\cite{adam}, with the momentum parameters set to $\beta_1=0.9$ and $\beta_2=0.999$.
Due to (TODO: citation needed), one can train \acp{RNN} in a more stable fashion by using a
learning rate scheduler. For this, I used a multi-step learning rate scheduler with milestones
at 4 and 7 epochs, and a stepsize of $\gamma=0.1$. This means that the learning rate for epochs
1 to 3 is $\eta$, from 4 to 6 is $\gamma\eta$ and from epoch 7 and beyond is $\gamma^2\eta$.
The maximum number of epochs was set to 40, but we also used an early stopper on the validation loss
with a patience of 5 epochs, so training could terminate earlier.
The details on how all of these hyperparameters were selected can be found in \cref{ch:hyp_amex}.

\subsubsection{Evaluation metrics}%
\label{ssub:Evaluation metrics}

%%% The metric used %%%
The Amex metric, is 50\%-50\% weighted split between the normalized Gini coefficient, $G$ and
the default rate captured at 4\%, denoted $D$ \citep{amex-data}.

Assume made predictions $p_1,p_2,\dots,p_N$ for each of the $N$ customers, and assume these
predicted default probabilities are sorted in non-increasing order. Also assume they have
associated normalized weights $w_1,w_2,\dots,w_N$ that sum to 1.
Then we take the top predictions $p_1,p_2,\dots,p_\omega$
captured within the highest-ranked 4\% of our predictions, that is, setting $\omega$ to be
the highest integer such that
Then we get the default rate
\begin{equation}
    D= \frac{\sum^{\omega}_{i=1} y_i}{\sum^{N}_{i=1} y_i},
    \textrm{ where } \omega \textrm{ is the highest integer such that }
    \sum^{\omega}_{i=1} w_i \leq 0.04 %\cdot\sum^{N}_{i=1} w_i.
\end{equation}

% TODO: change notation to k_i or something to make it clear what the k does on the RHS of the
%       equation.
From \cite{gini}, we know the Normalized Gini coefficient can be computed as
\begin{equation}\label{eq:fdjf09j}
    G_k=2  \sum^{N}_{i=1} w_i \left(\frac{p_i - \overline{p}}{\overline{p}}\right)
    \left(\hat{F}_i - \overline{F} \right),
\end{equation}
where $\hat{F}_i=\frac{w_i}{2} +\sum^{i+1}_{j=1} w_i$ and $\overline{p}=\sum^{N}_{i=1} w_i p_i$ and
$\overline{F}=\sum^{N}_{i=1} w_i \hat{F}_i$.
To compute the normalized Gini coefficient, we first sort the predictions in non-decreasing order
by the true labels $y_1,y_2,\dots,N$, and consider $p_1,p_2,\dots,p_N$ and $w_1,w_2,\dots,w_N$
accordingly. Let $G_0$ denote the result of computing \cref{eq:fdjf09j} with this sorting.
Then we sort the values by the predicted probabilities $p_1,p_2,\dots,p_N$ in non-decreasing order.
Let the value of \cref{eq:fdjf09j} computed with this ordering be denoted $G_1$.
The normalized Gini coefficient is then $G=G_1/G_0$, which is what we use in the final metric
\begin{equation}
    \textrm{Amex metric}=\frac{1}{2} \left(G +D \right).
\end{equation}

We also consider the validation loss as one of our metrics.

% }}}

% Preprocessing method experiments
\subsection{Preprocessing method experiments}% {{{
\label{sub:amex_res}

For the preprocessing method experiments on the Amex dataset, we look at the performance of
the three novel methods I proposed in \cref{sec:EDAIN-method,sec:EDAIN-KL-method,sec:PREPMIX-CAPS-method}, that is, the \ac{EDAIN} method, the \ac{EDAIN-KL} method and the \ac{PREPMIX-CAPS} method.
All these methods were compared to the state of the art in adaptive dynamic preprocessing techniques
for multivariate time-series data, which includes the \ac{DAIN} method by \cite{dain} and the
\ac{BIN} method by \cite{bin}. Additionally, as a baseline method, we also compare the
aforementioned methods to standard scaling applied across time, as this is a standard
preprocessing procedure in many works (citation needed).
Before going into the results, we briefly go over the hyperparameters chosen for each of the
preprocessing methods.

\subsubsection{Learning rate tuning}%
\label{ssub:Learning rate tuning}

As described in \cref{sub:edain_opt_sgd}, when optimising the adaptive preprocessing methods
through stochastic gradient descent, we should select individual learning rates for each part
of the adaptive preprocessing layer, lest the convergence might be unstable.
As the \ac{DAIN} and \ac{BIN} layer has never been applied to the Amex dataset, these learning
rates need to be manually tuned. The details of how I performed this tuning is found in
\cref{ch:lr_tuning}.

% Move onto specific mentions of learning rates for the different layers
For the DAIN layer, the optimal learning rate parameter for the shift layer
and scale layer were found to be $\eta_{\textrm{shift}}=\eta_{\textrm{scale}}=1.0$. Recall that
the actual learning rate is this parameter multiplied by the base learning rate $\eta$.
Despite the \ac{DAIN} layer consisting of three layers, an adaptive shift layer, an adaptive
scale layer, and an adaptive gating layer, the adaptive gating layer is not used in my
experiments because \cite{dain} found in inclusion of the adaptive gating layer to decrease
performance when used together with an \ac{RNN} sequence model.
For the \ac{BIN} layer, the optimal learning rate modifiers for the different parameters were
found to be $\eta_\beta=10.0$, $\eta_\gamma=1.0$ and $\eta_\lambda=10^{-6}$.
For the \ac{EDAIN} layer, the optimal learning rate modifiers were found to be
$\eta_{\textrm{scale}}=10^{-2}$, $\eta_{\textrm{shift}}=10^{-2}$, $\eta_{\textrm{outlier}}=10^{2}$,
and $\eta_{\textrm{power}}=10$. Additionally, for this method, we only consider the global-aware
mode as the local-aware mode was designed to handle more multimodal datasets, of which the Amex
dataset is not. The local-ware \ac{EDAIN} method is thus not expected to outperform the global-aware
version in this case.

% Tuning for EDAIN-KL and PREPMIX-CAPS
The \ac{PREPMIX-CAPS} and \ac{EDAIN-KL} layers also have hyperparameters that need
tuning. With the \ac{PREPMIX-CAPS} routine, we need to select $k$. The optimal $k \in \mathbb{N}$
was found to be $k=20$, and the detail of how this was found is in \cref{ch:lr_tuning}.
Additionally, the statistics clustering method was used instead of the \ac{KL-divergence}-based
method, and the linkage criteria was set to \textit{average}. Additionally, the number of bins
was set to be $n_{\textrm{num. bins}}=5000$. For the \ac{EDAIN-KL} layer, the optimal learning
rate modifiers were found to be
$\eta_{\textrm{scale}}=10$, $\eta_{\textrm{shift}}=10$, $\eta_{\textrm{outlier}}=10^{2}$,
and $\eta_{\textrm{power}}=10^{-7}$.


\subsubsection{Overview of results}%
\label{ssub:amex-results}

% Amex convergence plot (a) loss (b) AMEX metric
% multi-figure {{{
\begin{figure}[htp]
    \centering
    \begin{subfigure}[b]{0.99\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/amex_performance_convergence.pdf}
        \caption{Average cross-validation loss of \ac{RNN} model after each training epoch.
        Lower is better.}
        \label{fig:amex_performance_loss}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.99\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/amex_performance_convergence_metric.pdf}
        \caption{Average cross-validation American Express competition metric of the \ac{RNN} model
        after each training epoch. Higher is better.}
        \label{fig:amex_performance_metric}
    \end{subfigure}
    \caption{
        The plots show the average cross-validation performance of different preprocessing methods
        applied to the American Express dataset when training a \ac{RNN} binary classification model.
        The cross-validation was done using five disjoint 20\% validation sets.
        The dot highlights the earliest epoch where all five models are deemed to have converged by
        the early stopper, and can be interpreted as the convergence speed.
    }%
    \label{fig:amex_performance}
\end{figure}
% }}}

% Amex performance table
% table {{{
\begin{table}[htp]
    \centering
    \begin{tabular}{lll}
        \toprule
        Method & Validation loss & AMEX metric \\
        \midrule
        Standard scaling & $0.2213 \pm 0.0039$ & $0.7872 \pm 0.0068$ \\
        PREPMIX-CAPS (k=20) & $0.2208 \pm 0.0033$ & $0.7875 \pm 0.0053$ \\
        EDAIN (global-aware) & $\mathbf{0.2199} \bm\pm \mathbf{0.0034}$ & $\mathbf{0.7890} \bm\pm \mathbf{0.0078}$ \\
        EDAIN-KL & $0.2218 \pm 0.0040$ & $0.7858 \pm 0.0060$ \\
        DAIN & $0.2224 \pm 0.0035$ & $0.7847 \pm 0.0054$ \\
        BIN & $0.2237 \pm 0.0038$ & $0.7829 \pm 0.0064$ \\
        \bottomrule
    \end{tabular}%
    \caption{
        Evaluation results using the American Express default prediction dataset. Lower validation
        loss and higher AMEX metric is better.
        All confidence intervals are based on evaluation metrics from 5 cross-validation folds,
        and are asymptotic normal 95\% confidence intervals.
        The experiment methodology, including a description of the metrics,
        is described in  \cref{sec:amex_meth}.
    }%
    \label{tab:amex_performance}%
\end{table}
% }}}

% Amex fold breakdown
% figure {{{
\begin{figure}[htp]
    \begin{center}
        \includegraphics[scale=1]{figures/amex_performance_convergence_per_fold}
    \end{center}
    \caption{Validation loss at convergence for each of the five folds in the American Express
    default prediction dataset.}
    \label{fig:amex_folds}
\end{figure}
% }}}

In \cref{fig:amex_performance}, we see the average validation loss and Amex metric for the
5-fold cross-validaiton experiments using the different preprocessing methods. From the plots,
we see that the global-aware \ac{EDAIN} out-performs all the other methods tested, both
when looking at the average validation loss and when looking at the average metric value.
However, it is slightly slower to convergence when compared to standard scaling,
\ac{EDAIN-KL} and \ac{BIN}, but not by many epochs.
% The subfigures:
% In \cref{fig:amex_performance_loss} and \cref{fig:amex_performance_metric},
In \cref{tab:amex_performance}, we show a confidence interval for the validation loss and Amex
metric, based on the 5 different folds. From the 95\% confidence intervals, we see that for both
the Amex metric and validation loss, that all the methods overlap. As such, from this table
alone, we cannot prove that the proposed global-aware \ac{EDAIN} is significantly better than
the competition. However, most of the variance comes from the folds themselves. Consider the
validation loss shown for each of the 5 folds in \cref{fig:amex_folds}. Here, we see that despite
the validation loss varying from fold-to-fold, the global-aware \ac{EDAIN} method beats the
competition on all folds. From this plot, we can also rank the methods with \ac{EDAIN} as the
best performing, \ac{PREPMIX-CAPS} in second place, and standard scaling in third place.
The performance difference between \ac{EDAIN-KL} and \ac{DAIN} is then somewhat overlapping,
but \ac{BIN} performs the worst out of the methods attempt on this particular dataset.

% TODO: also integrate comments from Francesco on the results plot in this section!

% }}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{FI-2010 Limit order book dataset}%%%  LOB dataset %%%
\label{sec:American Express default prediction dataset}%%%%%%%%

% Description
\subsection{Description}% {{{
\label{sub:Description}

% TODO: clean this up!
The second real-world dataset considered in our experimental evaluation is a publicly available,
large-scale dataset called FI-2010, which contains several \ac{LOB} records
\citep{lob-data}.
There are two types of limit orders: A bid order indicates the highest price at
which someone is willing to buy an asset, while an ask order is the lowest
price at which someone is willing to sell their asset \citep{gould2013limit}.
The FI-2010 dataset contains high-frequency \acp{LOB}, that is,
records of limit orders, from 10 business days in June 2010 \citep{lob-data}.
The data collected from five Finnish companies. Also
note that only the ten lowest and highest ask and bid order prices were recorded.
The data from \cite{lob-data} was cleaned and features were extracted based on the pipeline
proposed by \cite{lob_preprocess}, of which \cite{dain} has made publicly available%
\footnote{The preprocessed FI-2010 dataset can be found here: \url{https://github.com/passalis/dain}}.
This resulted in $N=453\,975$ vectors of dimensionality $d=144$. To
use window $T=15$.
The task is then to predict whether the \textit{mid price} will increase, decrease or remain
stationary in in $H$ timesteps after the window. The integer $H$ is called the
horizon, and to follow suite with the experiments of \cite{dain}, we use the horizon $H=10$.
The mid price is the average of  the highest bid price and the lowest ask price. We label a stock
as stationary if the mid price changes by less than 0.01\% in the horizon of $H=10$ timesteps.


% }}}

% Evaluation methodology
\subsection{Evaluation methodology}% {{{
\label{sec:lob_meth}

\subsubsection{Cross-validation}%
\label{ssub:Cross-validation}

We use the \textit{anchored cross-validation scheme}  proposed by \cite{lob-data}.
This consists of using the first day of data to train the model, then evaluate it on the second
day of data. Afterwards, the first two days of data is used for training, and the third day
for evaluation. This is repeated until we use the first 9 days of data and the 10th day for evaluation. This scheme is illustrated in \cref{fig:lob_anchor}.
In total, this means we evaluate the models and preprocessing methods in 9 different folds.

% This figure was produced by running
%     pdfcrop --margins '0 0 0 -377' lob_paper_p12.pdf lob_cross_val_diagram.pdf
% on a PDF copy of the 12th page of the LOB FI-2010 paper
\begin{figure}
\begin{center}
    \includegraphics[width=\textwidth]{diagrams/lob_cross_val_diagram.pdf}
\end{center}
\caption{Anchored cross-validation experimental setup framework}
\label{fig:lob_anchor}
\end{figure}

\subsubsection{Sequence model}%
\label{ssub:Sequencemodel2}

Used a similar \ac{GRU} model as with Amex dataset, but changed parameters to match \ac{RNN}
model used by \cite{dain} in order to get a fair comparison to their \ac{DAIN} methods, evaluated
on the same dataset. The architecture is  similar to the \ac{RNN} model used for the Amex dataset
and described in \cref{ssub:Sequence model}. However, instead of using two stacked \ac{GRU} cells,
we just have one with 256 units. The classifier head that follows then consists of one linear
layer with 512 units, followed by a \ac{ReLU} layer and a dropout layer with probability $p=0.5$.
The output layer is
a linear layer with 3 units, as we are classifying the multivariate time-series into one of
three classes, $\mathcal{C}=\{\textrm{decrease}, \textrm{stationary}, \textrm{increase}\}$.
These outputs are then passed to a softmax activation function such that the
output is a probability distribution over the three classes and sums to 1.
The softmax activation function is defined as
\begin{equation}
    \left[\textrm{softmax}(\bfy)\right]_j=\frac{e^{y_j}}{\sum^{k}_{i=1} e^{y_i}} ,\qquad j=1,2,\dots,k.
\end{equation}
In our case, we have $k=3$.

\subsubsection{Optimising the model}%
\label{ssub:as8dhasdh}

The targets are ternary labels $y_1,y_2,\dots,y_N \in \{0,1,2\}$, denoting whether the mid-price
decreased, remain stationary, or increased. The output of the model is, as discussed above,
probability vectors $\mathbf{p}_1, \mathbf{p}_2,\dots, \mathbf{p}_N \in
(0,1)^3$. Therefore, a suitable loss function is the cross-entropy loss
function, defined as
\begin{equation}
    \mathcal{L}(\mathbf{p}_i, y_i)=-\sum^{2}_{c=0} \mathbb{I}\{y_i=c\} \log \left( p_{i,c} \right),
\end{equation}
where $p_{i,c}$ denotes the predicted probability of class $c$ for the $i$th input sample.
The \ac{GRU} \ac{RNN} model was then trained according to the description in
\cref{sec:Deep learning}. This was done using a batch size of 128. The optimizer used
was the RMSProp optimizer proposed by \cite{rmsprop}. The base learning rate was set to
$\eta=10^{-4}$. No learning rate scheduler nor early stoppers%
\footnote{%
    Despite not using any early stoppers,
    all the metrics were computed based on the model state at the epoch where the validation loss
    was lowest. This was due to
    the generalization performance starting to decline in the middle of training in most cases.
}
were used in order to best reproduce
the methodology used by \cite{dain}. At each validation folds, the model was trained for 20
epochs. The details on how all these hyperparameters were tuned can be found in
\cref{ch:hyp_lob}.

\subsubsection{Evaluation metrics}%
\label{ssub:Evaluation metrics}

For evaluating the model performance on the FI-2010 \ac{LOB} dataset, we consider look at the
macro-$F_1$ score and Cohen's $\kappa$ metric.
Recall that we have three classes for our true labels,
$\mathcal{C}=\{\textrm{decrease}, \textrm{stationary}, \textrm{increase}\}$.
We consider the predicted label to be the entry with the highest probability in the predicted
probability vector $\mathbf{p} \in (0,1)^3$
Then, let $\textrm{TP}_c$, $\textrm{FP}_c$, $\textrm{TN}_c$
and $\textrm{FN}_c$ denote the true-positive, false-positive, true-negative and
false-negative rates for each class $c \in \mathcal{C}$. We then have that the precision and
recall for a class $c \in \mathcal{C}$ is defined as
\begin{equation}
    \textrm{precision}_c= \frac{\textrm{TP}_c}{\textrm{TP}_c+\textrm{FP}_c}
    \qquad \textrm{recall}_c=\frac{\textrm{TP}_c}{\textrm{TP}_c+\textrm{FN}_c}.
\end{equation}
The $F_1$ score for a single class $c \in \mathcal{C}$ is then
\begin{equation}
    F_1^{(c)}=2\frac{\textrm{precision}_c  \cdot \textrm{recall}_c}{\textrm{precision}_c+\textrm{recall}_c} .
\end{equation}
The \textit{macro-}$F_1$ score metric is then simply the arithmetic mean of the $F_1$ score for
the different classes, that is
\begin{equation}
    \textrm{macro-}F_1=\frac{1}{|\mathcal{C}|}  \sum^{}_{c \in \mathcal{C}}  F_1^{(c)}.
\end{equation}

Cohen's $\kappa$ metric is computed as
\begin{equation}
    \kappa=\frac{p_o-p_e}{1-p_e} \in [-1,+1],
\end{equation}
where $p_o$ is the \textit{observed} probability of agreement between the true label and the
predicted label, and $p_e$ denotes the \textit{expected} agreement between the true and predicted
labels, assuming they are independently resampled from a distribution matching the empirical
distribution of the true labels and predicted labels, respectively \citep{kappa}.
Thus, this metric measures the level of agreement between the predicted and true labels,
accounting for agreement by chance.

% }}}

% Preprocessing method experiments
\subsection{Preprocessing method experiments}% {{{
\label{sub:Preprocessing method experiments}

TODO: make this similar to Amex discussion part

\subsubsection{Learning rate tuning}%
\label{ssub:Learning rate tuning}

Mention that use learning rate tunes from papers, but did it mnaually for BIN because not
present in their paper.

\subsubsection{Overview of results}%
\label{ssub:Overview of results}

% LOB performance table
% table {{{
\begin{table}[htp]
    \centering
    \begin{tabular}{lll}
        \toprule
        Method & Cohen's Kappa, $\kappa$ & Average $F_1$-score \\
        \midrule
        Standard scaling & $0.2772 \pm 0.0550$ & $0.5047 \pm 0.0403$ \\
        Min-max scaling & $0.2618 \pm 0.0783$ & $0.4914 \pm 0.0603$ \\
        BIN & $0.3670 \pm 0.0640$ & $0.5889 \pm 0.0479$ \\
        DAIN & $0.3588 \pm 0.0506$ & $0.5776 \pm 0.0341$ \\
        EDAIN (local-aware) & $\bm{0.3836 \pm 0.0554}$ & $\bm{0.5946 \pm 0.0431}$ \\
        EDAIN (global-aware) & $0.2820 \pm 0.0706$ & $0.5111 \pm 0.0648$ \\
        EDAIN-KL & $0.2870 \pm 0.0642$ & $0.5104 \pm 0.0519$ \\
        \bottomrule
    \end{tabular}%
    \caption{
        Evaluation results using the FI-2010 limit order book dataset..
        Higher $\kappa$ and higher $F_1$-score is better.
        All confidence intervals are based on evaluation metrics from 9 cross-validation folds,
        and are asymptotic normal 95\% confidence intervals.
        The experiment methodology, including a description of the metrics,
        is described in  \cref{sec:lob_meth}.
    }%
    \label{tab:lob_performance}%
\end{table}
% }}}

% LOB cross-validation breakdown
% figure {{{
\begin{figure}[htp]
\begin{center}
    \includegraphics[width=\textwidth]{figures/lob_performance_per_fold.pdf}
\end{center}
\caption{Validation $\kappa$ and average $F_1$-value at convergence for each of the
    nine folds in the FI-2010 limit order book dataset.}
\label{fig:lob_folds}
\end{figure}
% }}}

In \cref{tab:lob_performance}, we see the average macro-$F_1$ score and Cohen's $\kappa$ metric
for the nine different validation folds, along with a 95\% confidence interval for each metric.
For both metrics, the different preprocessing methods' performance fall into two groups.
The first group consists of standard scaling, min-max scaling, global-aware \ac{EDAIN} and
\ac{EDAIN-KL}, all of which achieve similar performance with average $\kappa$ metric values
in the range 0.26-0.29 and macro-$F_1$ values around 0.49-0.51. The second group of methods,
involving \ac{BIN}, \ac{DAIN} and local-aware \ac{EDAIN}, all perform noticeably better than the
first group, with average $\kappa$ values in the range 0.36-0.38 and mean macro-$F_1$ scores around
0.58-0.59. This is likely because all the methods in the second group do
\textit{local-aware normalization} where the shift and scale depends on summary statistics computed
for each sample. None of the methods in the first group does this. This phenomenon will be
discussed in greater detail in \cref{sub:local_vs_global}.
% Mention how EDAIN ga and EDAIN-KL still better than ss
Within the first group, we notice that global-aware \ac{EDAIN} and \ac{EDAIN-KL} both slightly
outperform standard scaling. This is also somewhat evident in \cref{fig:lob_folds}, where
especially \ac{EDAIN-KL} beats standard scaling on most of the folds.
% Mention that EDAIN la still slightly better than BIN and DAIN
TODO TODO TODO TODO THIS NEXT JKJKJKJKJ

%%%%%%%%%%%%%%%%%%%%%%%%%%%%% THIS ONE NEXT %%%%%%%%%%%%%%%%%%%
% TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


. Validation split shown in \cref{fig:lob_folds}.


% }}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Discussion} %%%%    Discussion        %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TODO: introduction to this chapter

% EDAIN
\section{EDAIN}% {{{
\label{sec:EDAIN-discuss}

\subsection{Local vs.\ global normalization}%
\label{sub:local_vs_global}

Local good on LOB, global good on Amex.
% It’s to do with “local” vs “global” normalization. In amex data, batches aren’t
% really that different. They’re just “random samples” from a global
% distribution. Therefore, it makes more sense to aim for a global batch-agnostic
% normalization procedure, such as the EDAIN method I’ve implemented, and this is
% why DAIN and BIN does not work that well. On the other hand, for LOB dataset,
% have stocks from different companies and distributions are very multimodal,
% illustrating the difference in statistics between e.g. companies. On such
% problems, makes more sense to aim more more “local” mode-aware optimization,
% and reason why DAIN and BIN performs better

\subsection{Adaptive outlier removal}%
\label{sub:Adaptive outlier removal}

% Learns whether to apply a tanh activation or not, and as saw in \ac{PREPMIX-CAPS}, often mixture
% of tanh and standard scaler, so adaptive version???


% }}}

% EDAIN-KL
\section{EDAIN-KL}% {{{
\label{sec:EDAIN-KL-discuss}

% One advantage is that if DNN model changes, can still keep the bijector and just retransform
% the data later. Do not have to add overhead of adaptive layer like with EDAIN

% TODO: comment on why not performing very well:
% * Even worse than standard scaling.
% * TODO: come up with some hypothesis on why this does not work that well? Maybe work well on synthetic data???

% }}}

% PREPMIX-CAPS
\section{PREPMIX-CAPS}% {{{
\label{sec:PREPMIX-CAPS}

% TODO: comment on why might not be that much better than just standard scaling:
% * E.g. potential that all the important features are in one clustering, and other features
%   not that important, so does not matter what preprocessing method apply to them
% * But in the validation split method, see that outperforms standard scaling across time
%   on some of the splits, so somewhat good
% * Additionally, interesting to see the mixture of the preprocessing methods selected for
%   the method. See appendix ....


% }}}

% TODO:
% * Nice discussion point is how standard scaling across time outperforms the other standard scaling
%   method, and how this can be because the change in distribution from one
%   timestep to next conveys important information for each customer, so if
%   standard scaling across both time and dimension, then discard this
%   information
% * If I want to add this discussion point, I should also include the other
%   standard scaling method in my benchmarking experiments in the results chapter


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion} %%%%     Conclusion      %%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Summary
\section{Summary}% {{{
\label{sec:Summary}



Conclusion goes here.



% }}}

% Main contributions
\section{Main contributions}% {{{
\label{sec:Main contributions}

% }}}

% Future work
\section{Future work}% {{{
\label{sec:Future work}

% }}}

% Appendix
% Appendix {{{
\clearpage
 %% reset page counter and start appendix pages with A
\pagenumbering{arabic}
\renewcommand*{\thepage}{A\arabic{page}}

%% Appendix goes here
\appendix

\chapter{Hyperparameter selection for American Express experiments}
\label{ch:hyp_amex}

The \ac{RNN} architecture is based on a ``starter model'' found on the Kaggle competition
discussion page for the American Express default prediction competition, where the dataset
originate from \cite{amex-data}. The ``starter model'' was developed by \cite{amex-starter}.

\chapter{Learning rate tuning for adaptive layers}
\label{ch:lr_tuning}

TODO: add plot of learning rate tuning for \ac{PREPMIX-CAPS}, showing how $k=20$ is best.

\chapter{Learning rate tuning for LOB}
\label{ch:hyp_lob}

TODO

% }}}

% References {{{
%%References part of appendices
% References: modify the file refs.bib
\bibliographystyle{plainnat}
\bibliography{refs}


% }}}
\end{document}
% vim: set foldmethod=marker:
